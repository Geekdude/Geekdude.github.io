var store = [{
        "title": "Engineering in England",
        "excerpt":"A Weekend in Cardiff, Wales—June 15–17 (2013)  by Josh Penney and Aaron Young   Originally Posted on tickle.utk.edu.   During the long weekend, we took a train out to Cardiff, which is the capital of Wales. We picked Cardiff for several reasons, but mostly for the Doctor Who Experience, which is a museum about the BBC show “Doctor Who” which happens to be one of our all-time favorite shows.   As we continued to explore Cardiff and the surrounding area throughout the weekend, we discovered some amazing castles dotted throughout the Welsh countryside. We visited three castles, Cardiff Castle, Caerphilly Castle, and Castel Coch.   Each castle had its own special feel ranging from the very nicely refurbished Castel Coch (shown below), to the much more impenetrable fortress of Caerphilly Castle.    Wales provided a very different cultural feel than we had experienced in the city and the countryside definitely operated at a much slower pace than downtown London.   It was a great trip to get away from the hustle and bustle of the city and explore the rich, and extremely green, Welsh countryside.                                                                                   ","categories": ["Life"],
        "tags": ["Travel"],
        "url": "https://geekdude.github.io/life/engineering-in-england/",
        "teaser":"https://geekdude.github.io/assets/images/engineering-in-england-castel-coch.jpg"},{
        "title": "New Website using Jekyll",
        "excerpt":"I decided that my project over Christmas break would be to create a new website. Although setting up the website overflowed past Christmas break, it is finally live. What motivated my decision was finally figuring out how to get my website to show up in a Google search. Spoiler alert, I registered my website with Google Search Console.   The Old Website  My old website was fine, but nothing special. A picture of the old site is shown below:  The old website consisted of a single page and its content was the same as the new About page. There were a few reasons I wanted to update my website.     I am not very good with HTML and I would rather create content for the website using markdown.   The previous website used no CSS and only had static HTML pages. It would be difficult to maintain navigation and constant style if I added additional webpages.   I thought it would be cool to try writing a technical blog, since my workflow has greatly improved from reading other people’s technical blogs.   I heard about GitHub pages and wanted to try creating a Jekyll website. Yes I know I’m not hosting this site on GitHub pages, but I will likely transfer my website there once I can no longer host my website on the University of Tennessee’s servers.   Why Jekyll?  So why did I go with Jekyll? For a few reasons. First, I wanted my website to be static, and I wanted a tool to help generate the static pages. Second, I learned about GitHub pages, which made me want to try out Jekyll so that I could possibly use GitHub pages for future project pages. And finally, Jekyll seems to have all the features I could want with great community support, which means that I would be able to find resources to help me to get started.   Learning Jekyll  I started learning Jekyll by reading the documentation provided on the Jekyll Website. I got the default template website running with the quick start guide and then worked my way through the step by step tutorial. The tutorial was well put together, but a little overwhelming since it is written for someone with web developing experience. It did help me figure out roughly how Jekyll generates the web pages, and made me realize that I wanted to use an existing theme instead of trying to build my own. The other main documentation for Jekyll is well put together and easy to navigate once the terminology is understood.   Because it was hard for me to see how to use Jekyll for my project based on the documentation, I also read other people’s blog posts which detailed how they went about creating their Jekyll websites. The two that I found most helpful are Create your Academic Website in Jekyll and Host it on GitHub by Steven Miller and Build your own website (with Jekyll and Minimal-mistakes theme) by Li Zeng. You might notice that my website looks very similar to Li Zeng’s and that is because I also decided to go with the Minimal Mistakes theme.   Finding a Template/Theme  When I was deciding how I wanted my website to look and be structured, I searched the web to try to find examples of personal websites that I could use for inspiration. Many of the sites I found were primarily used as portfolios and therefore showcased the work or the writings of the person. I knew that my website only needed a landing page, about page, blog (If I thought I would create posts), and a link to my resume. The key to the content I wanted is that it would consist of multiple pages and would be set up as a personal academic website. This ruled out all the themes which focused on a single landing page or didn’t fit the personal website aesthetic.   When searching for a theme, I looked through jekyllthemes.org and jekyllthemes.io. I preferred jekyllthemes.org since all the themes are free, however, some of the themes have bad links. Jekyllthemes.io was also good to look through and you can filter the results by free websites and all the themes had good links. Many of the better looking themes can be found on both sites. I found 7 themes that caught my attention as potential candidates. Then I narrowed the search down to two favorites. The final two were Minimal Mistakes and indigo. I ended up choosing Minimal Mistakes, since I liked the more content heavy splash page layout for a homepage, and the theme seemed feature rich and well-designed. It got bonus points for the well laid out documentation that was written using the theme. Indigo was close second since it is also well-designed and a beautiful minimal design with a very clean landing page.   Although there are differences in the parameters used between themes, the content for the site is all in markdown. So with some time invested in configuring a new theme, I could migrate my site to a different theme. The downside is that, since the themes have different configurations and layouts, different themes are not interchangeable without some work.   Making Content  The first step I took was to read the documentation for Minimal Mistakes. The documentation is structured well and custom configuring the site was easy and fun. I then ported over my old website’s content to make the about page. I then spent some time learning how to embed the resume document into the webpage. Next, I structured the home page and selected graphics to use. I found two useful resources for images. Font Awesome has a wide range of free SVG icons. I used their envelope icon for my contact me graphic. The second resource is Unsplash which has free high resolution photos. Unsplash is where I got the typewriter image I used for the resume graphic. That’s about it, the rest of the website setup was done following the Minimal Mistakes documentation.   Tada Done  So now I have a new website. Thank you for visiting and reading about it. If you have any questions or comments about the new website, please don’t hesitate to leave a comment below.  ","categories": ["Tech"],
        "tags": ["Website"],
        "url": "https://geekdude.github.io/tech/new-website/",
        "teaser":"https://geekdude.github.io/assets/images/old_website.png"},{
        "title": "Challenges with New Website Deployment (Jekyll and Subdomains)",
        "excerpt":"When I was deploying my website to the web server, I ran into a few challenges.  I will talk about them here in case anyone happens to run into similar  challenges.   SVG Graphical Issues  One issue that stood out was how chrome was rendering my SVG graphics. I  created the graphics with Inkscape and when they were displayed through Chrome,  some of the graphics had noticeable issues. The main problem was unsupported  fonts and Inkscape specific SVG features. I assumed the problems would only be  worse if I tested with multiple browsers, so to fix the issue, I converted all  of my SVG graphics to PNG images. I figured that the PNG images would be able  to be rendered properly regardless of the browser. Although this means that the  quality of the vector graphics is reduced, I think it is still a better  solution than having graphics that look incorrect. I could take the effort make  sure I only use standard SVG elements, that any viewer should recognize, but  that seemed like too much work to get right when PNG images already work well  enough.   Deploying to a Subdomain of a Site  The biggest issue came from the fact that I deployed the website to a subdomain  of the site (i.e. my site lives at http://web.eecs.utk.edu/~ayoung48/ and not  http://web.eecs.utk.edu/). This resulted in all of my assets referenced from  markdown pages resulting in broken links, as the browser would try to find the  asset at the root of the site and not at the root of the subdomain  /~ayoung48/. However, I couldn’t just append the subdomain to each of the  hyperlinks, as doing so would break the ability to test the site locally.  Luckily Jekyll already has a solution for this problem. Since the situation of  deploying to a subdomain is less common, it was harder to find good information  explaining the solution.   The first website I used to fix the issue was the Minimal Mistakes  Configuration Documentation. The configuration setting I needed to set was the site-base-url. This setting  specifies the subdomain of the site the website is deployed to. In my case, the  site-base-url is /~ayoung48. With this configuration change, the correct  subdomain is used when I serve the website locally for testing.  Parker  provides a brief explanation into base url, if you would like further  clarification. This change still didn’t fix the links, but resolved the local  deployment testing.   To fix the links, I had help from the liquid  filters and liquid  tags documentation pages. To  summarize, there are multiple was to have liquid fill in the correct url (with  the baseurl) for you. I’m going to explain each method using the old website  image found in the previous post. One method is to use the prepend filter:   ![Old Website]({{ '/assets/images/old_website.png' | prepend:site.baseurl }})  This method is OK, it just prepends the string found in site.baseurl to the  url, but there are better ways. Another way is to use the relative_url or  absolute_url filters.  ![Old Website]({{ '/assets/images/old_website.png' | relative_url }}) ![Old Website]({{ '/assets/images/old_website.png' | absolute_url }})  With these filters relative_url will generate  /~ayoung48/assets/images/old_website.png  and absolute_url will generate  https://web.eecs.utk.edu/~ayoung48/assets/images/old_website.png  I like these filters better than the prepend filter, since they give a clearer  intent on what you are trying to accomplish.   The last way I found to generate the links is with the link tag.  ![Old Website]({{ site.baseurl }}{% link /assets/images/old_website.png %})  Although this syntax is the worst looking, it gives the added benefit of  correctly generating the permalink of the file you are trying to link to. This  means that it is the preferred method for linking to other generated pages,  such as posts, since the permalink generation style could change. With this  method the link will still work correctly if the permalink style is changed.  This method also has the added benefit of performing link validation when  Jekyll builds the site. If you use the link or post_url tags, Jekyll will  check to make sure the link or post exists during the build process and throw  an error if the link is bad. Since tags provide correct generation of  permalinks and perform link validation, they are my preferred method for  specifying local links in my website.   Width Issue of Facebook Comments  Another issue I ran into was that on one computer, for whatever reason, the  Facebook comments would not take up the correct width. The fix for this problem  was to copy some css code I found on the internet to /assets/css/main.scss.  This solution was found on  stackoverflow.  // Fix width issues of Facebook comments. // https://stackoverflow.com/questions/22243233/how-to-make-facebook-comment-box-width-100-2014 .fb_iframe_widget_fluid_desktop, .fb_iframe_widget_fluid_desktop span, .fb_iframe_widget_fluid_desktop iframe {             max-width: 100% !important;             width: 100% !important;  }   Deploy Script  This last section isn’t really a deployment issue, but a way to make deploying  the website easier. I created a simple script to build the Jekyll site and copy  the files to the server.  #!/bin/bash #------------------------------------------------------------------------------- # Script to deploy the website to the server. #-------------------------------------------------------------------------------  # Variables -- Change to the correct values before use. instancehost=\"machine_address\" remotewebroot=\"webroot_folder\"  # Build Jekyll Site JEKYLL_ENV=production bundle exec jekyll build chmod -R a+rX _site  echo \"rsync to SSH host $instancehost ...\"  # Long form of rsync options # rsync --verbose --recursive --compress --checksum --human-readable --perms --delete-after` rsync -vrzchp --delete-after _site/ $instancehost:$remotewebroot   Update on 1/14/2020 for Jekyll 4.0  With the release of Jekyll 4.0, the link and post_url tags no longer need site.baseurl prepended every time they are used. These tags now use their relative_url filter to correctly take care of prepending the site.baseurl. This does mean that existing uses of the prepend pattern will break and the site.baseurl part should be removed. I like this change since it makes linking shorter, but a site wide find and replace was needed to fix the links.   So in summary, the recommended was to generate links is with the link tag. For example:  ![Old Website]({% link /assets/images/old_website.png %})  This method is now the most concise, readable, and will correctly generate the permalink URL following the site rules. It will also perform link validation to make sure the linked file exists.   ","categories": ["Tech"],
        "tags": ["Website"],
        "url": "https://geekdude.github.io/tech/new-website-challenges/",
        "teaser":"https://geekdude.github.io/assets/images/old_website.png"},{
        "title": "Skip Barber 3-Day Racing School",
        "excerpt":"Recently my Dad and I attended a Skip Barber 3-Day Racing  School at Road  Atlanta in Georgia. The trip was a blast and driving on the track in a Mustang  GT was exciting. Before we  left to go on the trip, I practiced in my Dad’s simulator to get a good feel  for the racing line around the track. I had previously done a lead follow at  Road Atlanta in street cars, but this was my first time driving a race car, and  my first time to drive around a track on my own. The cars were setup as race  cars. They had a racing air intake, a racing transmission, racing brakes and  suspension, and they had a roll cage with a five point harness. The car I used  was number 18.      At the racing school they taught us everything we would need to know to  understand car dynamics and to drive around a track safely and quickly. Each  day was broken up into two halves, before and after lunch. Each half then  consisted of in-classroom learning followed by a change to put what we learned  to test-driving the car. During the morning session of the first day we learned  about car mass, tire load, and various types of oversteer and understeer, and  the various causes of each. We also learned how to use our eyes to look where  we want to go and look further down the track to plan the next move while the  current one is being executed. The classroom lecture was quite entertaining and  the instructor intermingled the lesson with various stories. After the  classroom lecture, we were split up into two groups, red and blue. Each group  had 5 people in it. They red group first went to the autocross track and  practiced using our eyes to drive the car smoothly around the track. Cones  where setup for the turn-in, apex, and turn-out for each of the turns. The  instructors rode along with us and gave us advice as we navigated the autocross  course. After that, we went to the skid pad and learned what oversteer and  understeer feel like. We also practiced CPR (Correct, Pause, Recover) to  prevent the car from spinning out of control when oversteer happens. I was  actually fairly good at recovering the car, and sliding the car around was a  blast. One of the instructors had me try to drift the car under power once I  had succeeded in recovering the slides without throttle. Balancing the throttle  to continue the slide made recovery much harder.   During the afternoon session day 1, we learned how to downshift with the  toe-heel technique. We also discussed the racing line more and covered how to  take various turns. Then we left the classroom and practiced downshifting with  an exercise they had set up in the pit area. We also rode around the track in a  van to learn the racing line. We had to buckle up in the van since they took it  around the track at a good pace. After being show the track in the van, the two  groups took turns driving in lead follow sessions with a pace car and riding in  the pace car. After the first session the instructors stopped riding in the  race cars and all the driving was done solo. The lead follows were fun and  riding in the pace cars gave you a chance to hear commentary from the  instructor and see the race line further. The last turn I took in the pace car,  the instructor was really pushing the rental car. He was squealing the tires  and taking all the curbs. He knocked over some cones marking the turn apex.         The second day, in the morning, the lecture covered more information on grip,  turns, and flags. On the track, we did more lead follow and also took a turn  driving the race line in the rental car with the instructor and other students  riding along. After this, we started driving around the track solo with a stop  box setup. We would drive around the track and then stop on the back straight  to get feedback over the radio on how we did on the turns and if there is  anything to focus on improving the next lap. Initially there was a low speed  limit and rpm limit, but as we continued driving the speed limit and rpm limit  were slowly relaxed.   In the afternoon of the second day, we discussed braking and the friction  circle. After which, we went to the track and did braking exercises at turn 10.  With braking you put strong initial force on the brake and then slowly release  the force. ABS should engage or be just before engaging as your are braking.  Then during the turn you have a slight amount of brake still applied which is  known as trailing brake. If done correctly, your keep the tires at maximum  traction throughout the entire turn. After the exercise, we continued to do  stop box lapping sessions. When it was the other groups turn to drive, we would  travel to one of the flag stations along the track and watch the other group  drive. It was fun to see different views of the track and to critique the other  group to learn how we could better drive when it was our turn again.      The last day, in the morning, the lecture covered passing technique and driving  in the rain. On the track, we drove a passing exercise at turn 10, where we  drove off of the normal line to complete a pass. We then proceeded to do more  stop box lapping sessions with the speed increased.   In the afternoon, we learned how to conduct race starts. On the track, we  practiced a real race start. We did three starts, two double file starts and  one single file start, then transitioned into an open lapping session. The race  starts where intimidating since you where surrounded by other cars, but the  exercise itself wasn’t that bad. You raced from when the green flag was thrown  on the front straight away and stopped racing at the first turn. The open lap  sessions are when the stop box is taken down, and you drive around the track  until the checkered flag is shown. Passing is done by point by. If you see a  car behind you, your point them by on one of the straightaways. All that means  is your point which side of the car you want them to pass your on and then you  slow up a little, so they can complete the pass. After we finished driving our  last open lapping session, we were taken on a hot lap by one of the  instructors. And let me tell you that the instructors went much faster than I  was able to. I enjoyed going a comfortable pace around the track. After the hot  lapping sessions, we went back to the classroom and had a graduation ceremony.  I now have a diploma from the racing school, and I am eligible to apply for a  SCCA racing license. More importantly, I can now participate in driving on the  track for the One Lap of America driving  competition I am doing with my Dad later in the spring.     ","categories": ["Life"],
        "tags": ["Driving"],
        "url": "https://geekdude.github.io/life/skip-barber-racing-school/",
        "teaser":"https://geekdude.github.io/assets/images/road-atlanta-with-car.jpg"},{
        "title": "One Lap of America 2019",
        "excerpt":"Recently my dad and I competed in Brock Yates’ One Lap of America driving  competition. The official website for the event is found at  onelapofamerica.com. Since the event was  technically a competition, the events were time trials. Each team was scored  and ranked based on how fast they completed each event. The results and  rankings from the event can be found on the results  page. The car we used for the event is my dad’s 2017 Chevrolet Stingray. This put us  in the SGT-1 BB and Stock - GT Classes, where we placed 17th and  15th respectively. Our overall standing was 63rd.   Our goal, however, was not to place well, but to have fun, complete the event,  and bring the car back home, and we completed all of these goals. The  competition was long, but fun. Each day had roughly the same layout. We would  wake up early, around 7, then head to the track, where we would drive in two  events. After which, we would then drive for around 500 miles to reach the next  hotel, so that we could do it all again. The complete schedule for the event is  show below.                                                Day 0 Travel  The first day we packed up the car and traveled to the first hotel. We knew we were at the right place when we saw a viper with one lap stickers  and a Lamborghini next to it.                                                                                      Day 0 Gallery.       Day 1 Registration  The first day of the event we added stickers to our car, went through tech  inspection, walked around and looked at all the other cars, received our  information book, and attended a drivers meeting.                                                                                                                          Day 1 Gallery.       Day 2 Skid Pad and Autocross, South Bend and Kokomo, Indiana  The first day of driving, we started off at the Tire Rack Headquarters where we  drove on a wet skid pad. The goal was to see how much G-force you could put  into turning.   After the skid pad, we then went to Grissom Air Force Base where my dad drove  the autocross event. Before the event we both walked the course many times and  discussed strategies on how to drive it.   We closed off the day by driving from Indiana to our hotel in Ohio.                                                                                      Day 2 Gallery.                Day 3 Nelson Ledges, Garrettsville, Ohio  The second day of driving, we were at Nelson Ledges Road course. We got their  early enough that we were able to walk around the course before the track went  live. There was an old bridge that went over the starting line. The bridge  would shake when cars passed under it. My dad drove the course, but there were  parade laps after lunch that I drove.                                                                                                                                                              Day 3 Gallery.                Day 4 Road America, Plymouth, Wisconsin  The next track was Road America. In the morning there was rain and the track  was very slippery. There were some wreaks as cars slid off of the track, so our  run group at the end of the pack was delayed until after lunch. Going after  lunch had the great benefit of allowing the track to dry and the sun to come  up. This track was very large and there were many walking paths around the  track and places to spectate.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Day 4 Gallery.                Day 5 Brainerd International Raceway, Brainerd, Minnesota  Day 5 was one of the nicest, sunniest, and longest days of the trip. In the  morning I started to walk the course, but I only made it down the drag strip  portion of the front straight before I realized just how long the total course  would be. This track also had great places to spectate. The morning event was  the large full course and I spectated the driving from the top of the sky box  building. The track had a tunnel under the straight to cross from one side of  the track to the other.   In the afternoon, we drove on the shorter SCCA road course portion of the  track. This section of the track also had a rooftop to spectate from. The  shorter course was fun to watch, since you could see for a greater percentage  of the track and close to the start/finish line were fun turns to watch.  Travis Pastrana was fun to  watch go around one of the turns in particular. He would power slide each time  he went around. People spectating closer said that he was waving to the crowd  of people watching while doing the power slide. In the morning run, he was  talking on the phone with his teammates in the stands as he was driving around  the track. It was great fun to hear his conversations/interviews and watch him  drive.   Next we did drag racing on the front straight. The first time on the drag  strip, you tried to see how fast you could go. Then they did a bracket race,  where cars would try to get as close to their first time as possible without  going over their previous time. In the bracket, they would start the cars at  different times so that they should arrive at the finish line at the same time  (assuming they ran the same time as before).  Finally to finish off the day, we  had one of the longest transit drives of the trip, 555.5 miles.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Day 5 Gallery.                Day 6 Motorsports Park Hastings, Hastings, Nebraska  Hastings was a smaller course. We were able to walk the course in the morning.  The weather was rainy and windy, but there were shelters from the rain. They  had multiple covers and also a party bus. The party bus was nice since in  addition to blocking the rain, it would block the wind. Even with a fire suit  and multiple layers, it was cold this far north. This is also where our EZ pop  up canopy finally collapsed. It was starting to look rough, and had already  blown over once. This time we strapped it down really well with bags and  hammock straps, and instead of blowing away, it folded in on itself. However,  our stuff still stayed dry and we ended up not needing it for the rest of the  trip. This place also had the best food. They had only one thing on the menu  and it was a delicious brisket sandwich.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Day 6 Gallery.                Day 7 Blackhawk Farms, South Beloit, Illinois  Blackhawk Farms was a nice course. By now we had the rhythm down. Wake up, get  coffee, drive to course, walk course, morning session, lunch, afternoon  session, transit drive, Cliff Bars and gas station pizza for supper, check-in  and collapse in hotel bed. Repeat. By this point a nap was required, and I was  glad to have brought a hammock to nap in. This track has a second story  building to watch from, but the best place to watch was at some metal stands in  the U portion of the track.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Day 7 Gallery.                Day 8 National Corvette Museum (NCM) Motorsports Park, Bowling Green, Kentucky  I drove the events at NCM and it was a blast. The weekend before I went to a  Chin Track Days here, so I already knew the course and had multiple sessions of  experience driving it. In the morning, we first drove the east course, then  afterwards we moved directly into line to drive the west course. Driving the  split up course was new, but pit-in and pit-out was easy to find, and we looked  for them when we walked on the course in the morning. I was able to run good  times and I felt more confident on the track than I did during the week before.  The previous experience helped tremendously. The sink hole portion was the most  fun, and I was able to squeak the tires for most of the entrance turn. My time  on the west course was comparably better, but the straights were to the  advantage of our car. For lunch they had hibachi from a food truck. I was happy  to have Asian after all of the BBQ I had been eating earlier in the week.   After lunch we drove on the full course. I enjoyed the full course and I was  back in my comfort zone driving the same configuration I had during Chin. The  laps were good and the only sad part was I forgot to hit record on the car’s  race cam, so I don’t have any footage from that run. And without any proof to  the contrary, those were my best laps on the track ☺.   After the event, there was still time left for the track rental, so they opened  up the track for open lapping. My dad was finally able to drive on the track at  speed. Previously he had only done a parade lap on the course. This was the  last track day, but we still had to drive back up to Indiana for the skid pad  part 2, banquet, and group photo. So after being in Kentucky (closest  destination to home) we heading back north for the last day.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Day 8 Gallery.                Day 9 Skid Pad, South Bend, Indiana   First we line up all the cars to take a group photo. Then I got a chance to  drive the skid pad again, but this time on a dry track. I did much better in  the dry and I had more confidence to go fast. I think I was much closer to the  limit of the tires this time around and the car was sliding around more. For  this event they ran the cars in reverse order in the standings, and even so, it  was great to hear that I took first place on the skid pad after completing my  run. Finally the event ended with an awards banquet where the food was good and  I learned what a shoey  was.   And so after the banquet our trip was complete. We had successfully made it  around One Lap of America. Tired but happy, we then made our last transit drive  back home. At this point we were pros at long drives and we made it home  safely.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Day 9 Gallery.                ","categories": ["Life"],
        "tags": ["Driving"],
        "url": "https://geekdude.github.io/life/one-lap-of-america/",
        "teaser":"https://geekdude.github.io/assets/images/one-lap-day-1-left.jpg"},{
        "title": "TENNLab---Neuromorphic Architectures, Learning, Applications",
        "excerpt":"I am part of the TENNLab—Neuromorphic Architectures, Learning, Applications research group at the University of Tennessee. We are researching a new paradigm of computing, inspired by the human brain. Our research encompasses nearly every facet of the neuromorphic area, including current and emergent hardware implementations, theoretical models, programming techniques and applications. Specifically, I have worked on framework development, the DANNA and DANNA2 digital neuromorphic processors, and neuromorphic application development. I have also helped with the development of multiple neuromorphic robots, including NeoN (Neuromorphic Control System for Autonomous Robotic Navigation), GRANT (Ground-Roaming Autonomous Neuromorphic Targeter), and SABR (Self-Adjusting Balancing Robot). My focus has been on communication between traditional computers and neuromorphic processors. I have developed the NACC (Neuromorphic Array Communications Controller) to support high-speed, low-latency communication between the host PC and the neuromorphic system. I have also designed and built SNACC (Scaled-up Neuromorphic Array Communications Controller), which uses a NACC to connect multiple neuromorphic processors together for the purpose of building a large unified neuromorphic system. This system is designed to run large neural networks and also supports real-time communication with the Host PC.   SNACC     NACC     DANNA2 Viz    ","categories": ["Research"],
        "tags": ["Neuromorphic Computing"],
        "url": "https://geekdude.github.io/research/tennlab/",
        "teaser":"https://geekdude.github.io/assets/images/tennlab-poster.png"},{
        "title": "A Review of Spiking Neuromorphic Hardware Communication Systems",
        "excerpt":"Aaron R. Young, Mark E. Dean, James S. Plank, and Garrett S. Rose   September, 2019   IEEE Access   https://ieeexplore.ieee.org/document/8843969   View Article   Abstract  Multiple neuromorphic systems use spiking neural networks (SNNs) to perform computation in a way that is inspired by concepts learned about the human brain. SNNs are artificial networks made up of neurons that fire a pulse, or spike, once the accumulated value of the inputs to the neuron exceeds a threshold. One of the most challenging parts of designing neuromorphic hardware is handling the vast degree of connectivity that neurons have with each other in the form of synaptic connections. This paper analyzes the neuromorphic systems Neurogrid, Braindrop, SpiNNaker, BrainScaleS, TrueNorth, Loihi, Darwin, and Dynap-SEL; and discusses the design of large scale spiking communication networks used in such systems. In particular, this paper looks at how each of these systems solved the challenges of forming packets with spiking information and how these packets are routed within the system. The routing of packets is analyzed at two scales: How the packets should be routed when traveling a short distance, and how the packets should be routed over longer global connections. Additional topics, such as the use of asynchronous circuits, robustness in communication, connection with a host machine, and network synchronization are also covered.                     Graphical summary of neuromorphic hardware communication systems. The top half of this figure summarizes the routing schemes used by the neuromorphic systems, and the bottom half summarizes the routing methods.        Citation Information  Text   author    A. R. Young and M. E. Dean and J. S. Plank and G. S. Rose title     A Review of Spiking Neuromorphic Hardware Communication Systems journal   IEEE Access volume    7 pages     135606-135620 year      2019 doi       10.1109/ACCESS.2019.2941772 url       http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8843969&amp;isnumber=8600701   BibTex   @ARTICLE{ydp:19:ars,     author = \"A. R. Young and M. E. Dean and J. S. Plank and G. S. Rose\",     title = \"A Review of Spiking Neuromorphic Hardware Communication Systems\",     journal = \"IEEE Access\",     volume = \"7\",     pages = \"135606-135620\",     year = \"2019\",     doi = \"10.1109/ACCESS.2019.2941772\",     url = \"http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8843969&amp;isnumber=8600701\" }   ","categories": ["Research"],
        "tags": ["Neuromorphic Computing"],
        "url": "https://geekdude.github.io/research/ieee-access-2019-review-paper/",
        "teaser":"https://geekdude.github.io/assets/images/ieee-access-2019-graphical-abstract.png"},{
        "title": "The Big Orange Bramble",
        "excerpt":"This year marks the three-year anniversary of the 64 node Raspberry Pi cluster known as the Big Orange Bramble or BOB. I am happy to report that BOB is still running strong and there have been no major downtimes or issues with the system. BOB is still occasionally used to train neuromorphic networks using EONS, which you can read more about here. BOB was build as part of the special topics class ECE599 Supercomputer Design and Analysis, Summer 2016 of which I was part. This post includes images of the system and copies of the reports and presentations in case you want to read more about the system. The reports are also posted on Dr. Mark Dean’s website. Dr. Dean taught the course. Also included are the reports for the follow-up project named A Linux Integrated Computing Environment or ALICE, which used Pine64 boards and Nvidia TX1 GPUs.   Abstract   This project involved the design and construction of a high performance cluster composed of 68 quad-core ARMv8 64-bit Raspberry Pi 3s. The primary intent of the project was to establish the operating environment, communication structure, application frameworks, application development tools, and libraries necessary to support the effective operation of a high performance computer model for the students and faculty in the Electrical Engineering and Computer Science Department of the University of Tennessee to utilize. As a foundation, the system borrowed heavily from the Tiny Titan system constructed by the Oak Ridge National Laboratory, which was a similar but smaller-scale project consisting of 9 first generation Raspberry Pis. Beyond the primary target of delivering a functional system, efforts were focused on application development, performance benchmarking, and delivery of a comprehensive build/usage guide to aid those who wish to build upon the efforts of this project.   Picture Gallery                                                                                                                                                                                                                                                                                                              BOB Gallery.       Project Reports  BOB Documentation   Applications for 68 Node Raspberry Pi 3 Education Cluster   Performance, Management, and Monitoring of 68 Node Raspberry Pi 3 Education Cluster: Big Orange Bramble (BOB)   ALICE Documentation   Project Presentations  Slides: BOB Presentation   Video: Big Orange Bramble (BOB) HW SW Systems Presentations   Video: Big Orange Bramble (BOB) App Presentations   Slides: ALICE Presentation  ","categories": ["Tech"],
        "tags": ["High Performance Computing"],
        "url": "https://geekdude.github.io/tech/bob/",
        "teaser":"https://geekdude.github.io/assets/images/bob-hardware-diagram.png"},{
        "title": "Reorder",
        "excerpt":"I recently created a reorder python script to rename, reorder, add, and remove files/directories in a directory with a numerical naming scheme.   A common file naming practice is to prepend a number to the filename so that the files appear in the order specified by the number. However, say you want to add, delete, or reorder these files, you would have to go through and update all the filenames which have numbers changed from the operation. Although renaming the files like this is not hard, it’s annoying to do when there are multiple files which need to be renamed. I ran into this issue recently while writing my dissertation; each chapter is broken into its own .tex file stored in the chapters/ directory.   chapters ├── 01-introduction.tex ├── 02-related-work.tex ├── 03-previous-work.tex ├── 04-goals.tex ├── 05-tools.tex ├── 06-danna2-work.tex ├── 07-snacc.tex ├── 08-challenges.tex ├── 09-applications-performance.tex ├── 10-accomplishments.tex ├── 11-future.tex └── 12-conclusion.tex   In order to insert a new chapter or delete a chapter, all the files which come after that chapter will have to be renamed. Further, more in the main .tex file I include each of the chapters with an include statement.   \\include{chapters/01-introduction} \\include{chapters/02-related-work} \\include{chapters/03-previous-work} \\include{chapters/04-goals} \\include{chapters/05-tools} \\include{chapters/06-danna2-work} \\include{chapters/07-snacc} \\include{chapters/08-challenges} \\include{chapters/09-applications-performance} \\include{chapters/10-accomplishments} \\include{chapters/11-future} \\include{chapters/12-conclusion}   This means I have to renumber the files in the directory and the files in the include statements. In order to make this operation easier, I created a reorder python script to easily rename, add, delete, and reorder the *.tex files in the chapters directory. This script opens up a text buffer, and you make the desired changes to the buffer; then the script will make the changes to the directory and update the file numbers.   Additionally, I modified my makefile for the  project to include a target to build a chapters.tex file with the proper include statements for the chapters found in the chapters folder. Now in main.tex I only need to \\input{chapters.tex}.   Now if I want to add a new chapter between previous-work and goals, as well as rename “previous work” to “prior work”, all I need to do is run reorder chapters. I am then presented with the buffer   01-introduction.tex 02-related-work.tex 03-previous-work.tex 04-goals.tex 05-tools.tex 06-danna2-work.tex 07-snacc.tex 08-challenges.tex 09-applications-performance.tex 10-accomplishments.tex 11-future.tex 12-conclusion.tex   I then change the buffer to   01-introduction.tex 02-related-work.tex 03-prior-work.tex new-chapter 04-goals.tex 05-tools.tex 06-danna2-work.tex 07-snacc.tex 08-challenges.tex 09-applications-performance.tex 10-accomplishments.tex 11-future.tex 12-conclusion.tex   Then I can rebuild the  document and the changes to the chapters will be included with a newly generated chapters.tex.   Much appreciation to Jonathan Ambrose who helped come up with the idea for the script, motivated me to write it, and added several additional features to the script.  ","categories": ["Tech"],
        "tags": ["File-Organization"],
        "url": "https://geekdude.github.io/tech/reorder/",
        "teaser":"https://geekdude.github.io/assets/images/reorder-teaser.png"},{
        "title": "Sourdough Bread",
        "excerpt":"My favorite type of bread is a fresh, warm loaf of sourdough. I can almost go through a whole loaf by myself when it is fresh from the oven. Sourdough bread also makes the best french toast and great bread crumbs for wiener schnitzel. This post will go over the basics to getting started making sourdough bread. It includes links to the resources I used to get started, my favorite recipes, and some tips and tricks I have learned over my years of baking and experimenting for fun.   Starter  At first I tried to make my own starter. This didn’t really work out all that well. I learned that you should not try to make a sourdough starter by initially adding in baking yeast. If you do this, you will not get the good sourdough flavor and it will be harder to form the symbiotic relationship between bacteria and wild yeast which gives sourdough its unique acidic flavor. After a first bland attempt and subsequent difficulty to get a starter to grow with wild yeast (i.e. with only flour and water). I decided to buy starter from King Arthur Flour. This starter worked much better. I bought it in 2013 and I have kept it going ever since. I keep the sourdough starter in a King Arthur Sourdough Crock. They also sale the starter in a set with the crock.   Starter Care  King Arthur Flour provides a great sourdough guide to learn how to get started with sourdough bread. It includes information on how to maintain a starter as well as how to bake bread. This is where I first started and I still use much of what I learned here.   I generally end up neglecting my starter for a few months at a time when I get busy and didn’t have time to bake. The starter will do well for this long in the fridge. Do not freeze the starter since doing so could harm the microbes. If I’m not planing on using my starer within the next few days, I will keep it in the fridge. The cold slows down the activity of the organisms which also slows down the rate they consume the flour. After a while, brown liquid will form on the top of the starter. This is expected and can be mixed back into the starter before feeding. If the starter ever shows signs of mold, has an orange or pink tint/streak or smells putrid then it went bad with harmful microorganism and you will want to throw it out. I don’t know how common this is as mine has never spoiled. The smell of sourdough is quite sharp but it should smell like beer or bread and not like spoiled food.   When I feed my starter I mix together a cup of starter, a cup of water, and a cup of flour. I usually use a scale which means I mix together 172 grams of starter, water, and flour. You can vary the ratios, but I found that the same amount of each works well. I aim for a pancake batter like consistency. Too much water and there isn’t as much food. Too much flour and the starter can expand outside the container and makes a mess. A nice balance will bubble, but not trap the air so that it expands.   There is a difference between a fed and unfed starter. A fed starer is active and bubbling. Typically, you can feed the starter and wait a few hours and it will be active. An unfed starter is straight from the fridge before feeding. If it has been a long time since I fed the starter, I will feed it, let it sit at room temperature for a day, feed it again, and wait for it to appear active before I consider it fed. I have made bread with both fed and unfed starter. The fed starter makes it easier to get fluffier bread but unfed still works.   Another great trick I learned is that you can easily transfer starter by taking some of it and adding lots of flour so that it forms a dry ball. The dry ball can then be stored in the top of a bag of flour you are taking on a trip. I have used this to transfer my starter when I traveled to visit relatives and I wanted to make sourdough bread for them. The bag of flour with dried starter in the top was much easier to transfer then the ceramic crock.   Sourdough Bread  Bread making is as much an art as a science. There are many factors at work to determine how the final loaf will turn out. These variables include the activity of the starter, the water-to-flour ratio, and the time and temperatures for the various fermenting, proofing, and rising stages. Although I started out following the Extra-Tangy Sourdough Bread recipe, I also started experimenting with this recipe to learn how different ingredients and techniques affected the final loaf. Now I still loosely follow the recipe, but I do it more from experience and practice then from precisely measuring everything. My goal was actually to simplify the process as much as possible, down to the key ingredients, understand the role these ingredients play and then understand the phases of the proofing process. I found that the bread mostly came out well every time and the variation was fun. My current bread relies on simple ingredients and gets the great sourdough flavor from only the starter and the proofing steps.   My approach to making bread has been shaped by reading a variety of books, reading internet posts, and from watching YouTube videos. Of particular note is In Celebration of Simplicity: The Joy of Living Lightly which is about living a life based on gospel simplicity through an extended metaphor with the simple ingredients in bread. This book inspired me to keep the ingredients simple and to understand the role of each. The bread chapter in Cooked: A Natural History of Transformation was very interesting and led me to experimenting with whole wheat sourdough bread and also to use wetter dough. The whole wheat was very challenging, but I could eventually make a good sandwich bread loaf. However, I could never get as yummy of an artesian loaf as I could with white flour. The introduction to Classic Sourdoughs has a great, concise, introduction to the art of sourdough bread making with information on the various factors and how they affect the final loaf.   My Method of Baking Bread  So I will now walk you through my process making bread, but for a recipe and more details instructions follow the Extra-Tangy Sourdough Bread recipe and start experimenting. I start off the day before by mixing together a small mason jar of water (2-ish cups), the leftover starter after feeding it, and a sifter full of flour (3-ish cups) in a large glass mixing bowl. I spend a while mixing this together until it is quite smooth 1-3 minutes. At this stage it should be uniform and still liquidy like pancake batter. You want it to be liquidy so that it won’t overflow the bowl overnight. Then let the bowl sit out on the counter until you go to bed, then move it into the fridge (I will leave it out all night if I used an unfed starter).   The next day I just add some salt, honey, and herbs. With these ingreadients, I stopped measuring the amounts percisely. Start with the amounts listed in the recipe, but after awhile you get a feel for the rough amounts and no longer have to measure precisely. I like using thyme or a mix of Italian herbs. For this part I take inspiration from Penelope Wilcock who says in In Celebration of Simplicity: The Joy of Living Lightly “When I add salt, yeast, honey and herbs into my bread, I just put in ‘some’. Approximate amounts will do, creating acceptable variants in the taste.” I no longer add sour salt to my bread, since I am able to get a strong natural sourdough flavor without it. At this point I also usually add in some olive oil. If you want artesian bread, then leave the olive oil out. But if you want sandwich bread with a softer crust include it. The oil will reduce the size of the air pockets and make the crust softer and better for sandwich bread. Again with the oil, I don’t measure precisely but pour in ‘some’ from my tin olive oil container. I probably add around a tablespoon or two. On the amount of oil to add, Penelope says “I put lots in.” I mix everything together, so that it is uniformly combined. Then I either dump it out onto a bed of flour or start adding flour while I am stirring. I mix the added flour in with the dough.   At this point I should probably let the dough rest for a bit to autolyse but normally I forget and just start with the kneading. While I knead I try to keep the dough as wet as I want to deal with, as I start to knead it using the “slap” kneading method. This method is shown in the King Arthur Kneading video and requires less effort than other methods and allows stickier dough to be handled. This main kneading needs to be long and I do it for at least 10 minutes. As you continue kneading the dough will start to get firmer and harder to knead. This means you are closer to finishing. I try to keep it sticky and will occasionally add flour to the outside to make it less sticky. While I am kneading it, I add more flour based on the constency of the dough. From Cooked: A Natural History of Transformation, the wetter the dough, the better for artisan breads. However, I like to find my happy middle ground with a sticky dough, but one that is dry enough to no lose its structure during the proofing process. With really wet dough, you should fold the dough as it rises. With dryer dough, this is not required. I would begin with the recipe and then start expirementing. If you want to track expirements with the amount of ingredients in the recipe, you can use the baker’s percentage, which measures the ratio of other ingreadetants by weight to that of the flour. Of great inportance is the dough hydration, which is the baker’s percentage of water. Once I am done kneading, I clean the large mixing bowl the dough was in, coat it with olive oil, then add the dough back to the bowl.      Then I move the dough to my bread proofing box and wait a while. You can easily make bread without a proofing box, but see more information about them here. I either try to fold the bread every 30 min to 1 hour, if I am doing a particular wet dough as this helps the gluten form better, or I just let it sit without attention. Like the recipe says, I wait anywhere between 2 and 5 hours as I watch it occasionally to see how it is rising. Once I am convinced it has risen well, I do the short knock back kneading for around 2 minutes, then I divide the dough into two halves, shape it, and either add it to a bread loaf pan or a brotform. Often I will do both to make a round artesian loaf and a sandwich loaf.      Then I let it rise in the proofing box for another 2 to 4 hours until it has risen. Then I preheat the oven and bake the bread. I spray the top of the bread with water (olive oil could also be used) and cut slits in the dough to let the air escape as it rises. I usually start the oven off at 500°F for the first ten minutes to help the bread rise quickly. Then I reduce the temperature to 425°F and add aluminum foil to prevent the crust from hardening as much as it finishes cooking for 15 to 20 more minutes.      The finished bread is great while it is still hot. The second loaf can be frozen if you don’t plan on eating it for a few days. The bread will last for about a week before it becomes stale. Slightly stale bread works great for french toast (even better than fresh bread in fact since it keeps together better as it soaks up the egg/milk mix); also any leftover bread which is stale can be blended into bread crumbs and stored in the freezer. For bread crumbs the bread either has to be stale or toasted in order to blend into good bread crumbs.      Waffles  My favorite waffle recipe is by far the Classic Sourdough Waffles or Pancakes from King Arthur Flour. These Waffles have a great sourdough taste to them. I have found that you can leave excess batter in the fridge and cook it subsequent days without issues. I would try to eat it up within three days though. I also found that the sourdough flavor is slightly stronger each day.         Bread Proofing Box  I wanted to try using a proofing box to see if it would improve the quality of my sourdough bread. However, I didn’t want to by a proofing box if it would not make a big impact. So when I was thinking of a final group project idea for ECE551—Digital System Design, I decided to make my own “Smart” bread proofing box with temperature and humidity control, built-in automatic schedule, and a web interface. The project was great fun and the final report can be found here. As part of the project, I also experimented with making bread with the bread box. I found that the flavors of the bread were better when the bread box was used. At this time I also decided to stop using sour salt in my bread, since I could get great natural sour flavors in my bread and I didn’t want to cover them up with the sour salt. The bread proofer we made for the class worked great, but I ended up getting a commercial bread proofer mostly since it can fold down easily and was made out of sturdier materials. The commercial bread proofer does lack a few of the features from the one I made. It doesn’t have the proofing scheduling/timing, humidity control, or a web interface. On the flip side the commercial one works well and folds down nicely. However, its heating element works much better than the heat lamp I used. The heat lamp would dry out the top of the dough.   This video shows the bread proofer I made in action.            ","categories": ["Life"],
        "tags": ["Food"],
        "url": "https://geekdude.github.io/life/sourdough-bread/",
        "teaser":"https://geekdude.github.io/assets/images/bread-header.jpg"},{
        "title": "PhD Dissertation and Artifacts Available",
        "excerpt":"My dissertation titled SNACC: The Scaled-up Neuromorphic Array Communications Controller is now available at https://trace.tennessee.edu/utk_graddiss/5843/.   I have also made available two general-purpose IP blocks written in VHDL that I created as part of my dissertation work. They are a custom Aurora acknowledgment automatic repeat request design available at https://github.com/Geekdude/aurora-ack and a AXI4-Stream clock converter design available at https://github.com/Geekdude/axi4-stream-clock-converter.   Recommended Citation   Plain Text:   Young, Aaron Reed, \"SNACC: The Scaled-up Neuromorphic Array Communications Controller. \" PhD diss., University of Tennessee, 2020. https://trace.tennessee.edu/utk_graddiss/5843   Bibtex:   @PhdThesis{y:20:snacc,     title       = {SNACC: The Scaled-up Neuromorphic Array Communications Controller},     author      = {Aaron Reed Young},     institution = {University of Tennessee},     month       = {May},     year        = {2020},     url         = {https://trace.tennessee.edu/utk_graddiss/5843/} }  ","categories": ["Research"],
        "tags": ["Neuromorphic Computing"],
        "url": "https://geekdude.github.io/research/dissertation/",
        "teaser":"https://geekdude.github.io/assets/images/phd-teaser.png"},{
        "title": "Moving Website from web.eecs.utk.edu to Github.io",
        "excerpt":"After graduating last May, I decided to migrate my website from hosting on the EECS servers at UTK to GitHub Pages. I always planned on migrating to GitHub pages post-graduation; that was one reason I chose to build my website using Jekyll. GitHub pages can host any static website, but it is designed to build and work with Jekyll sites automatically. I wanted to migrate my website properly with correct redirection and Search Engine Optimization (SEO). Overall the process was relatively easy, but I did run into some issues, primarily with the customizations I have done to the website. This post will cover the overall steps I took and also the issues and workarounds I encountered.   Moving the site to GitHub  Previously I have been hosting my repository as a private repo in bitbucket. I decided to move the repository to a public GitHub repo since I used GitHub pages for hosting. At first, I tried just to move my repository over directly and use GitHub’s built-in building of Jekyll. However, this auto-building feature only supports a subset of Jekyll plugins. The plugin I use for pagination, Jekyll Paginate V2, is not one of the supported plugins. This limitation means that the built-in building would not work for my site without changing the plugins I use. Therefore, I turned off the automatic Jekyll building by adding a .nojekyll file to the root of the repository.   I then tried to build the site automatically with GitHub Actions, but I had trouble getting the site to build using a docker image with the Jekyll build action. While looking into the build issues, I realized my website would not build with the newest versions of Jekyll and Minimal Mistakes. By finding a copy of the Gemfile.lock file I used to build the website with last, I was able to roll back to the old versions and build my website correctly again. Again, the issue was related to the custom template files I created to use Paginate V2 for my home pages and article pages. The trouble with Jekyll and extension versions leads me to an important lessons learned: Store the Gemfile.lock in version control so that you can diagnose issues caused by building with newer versions of the tools and extensions. I was lucky and could just pull the lock file from my backups which saved me extra debugging time.   Since I like the way my website looks and I didn’t want to spend extra time porting my template to the newer version, I decided to keep building the website locally and uploading the static site for hosting. Luckily GitHub Pages supports this flow. The static website can be uploaded to the main branch or a branch called gh-pages. Actually, the branch and path can be chosen under Settings -&gt; Options -&gt; GitHub Pages, so any location can be used for the source. At first, I tested uploading the built website to the main branch, and the website was hosted as expected. But I then decided to host the website’s source in the main branch and a gh-pages branch to host the built website. To make the whole process simpler, I wanted to update my deploy script to build and deploy the website to a branch in the repository. At first, I was hesitant to store the built files in the repo, but since the files are in an orphaned branch, they are separate from the main branch. The orphan branch (as opposed to a folder in the main branch) will make cleaning the files up easier, if they get too large.   Luckily updating the build script was easy since I found a Git Directory Deploy script that I could leverage to copy the built site directory to a separate orphaned git branch. Now my deploy script just calls a customized version of the git deploy script with the variables filled in for my site. Previously it would use rsync to copy the build files to the EECS servers website folder.   Since I decided to use the same local build then deploy setup, minimal changes were needed to the site itself. I updated the site URL in _config.yml as well as updated the previously mentioned deploy scripts. That was all that was required to migrate the site and start hosting on GitHub Pages. The source code for the site can be found at https://github.com/Geekdude/Geekdude.github.io.   Setting up redirection  The next step was to set up redirection from the old URL to the new URL. After looking into redirection, I discovered that I wanted to use redirection with the permanent redirection status (301) which indicates that the resource has moved permanently. From various posts, I learned that a .htaccess file is used on linux servers to specify server-level redirects. I had trouble finding the correct redirect commands until I read the Apache Documentation for Redirect and RedirectMatch. The trouble I ran into was how to correctly map from a folder on a domain to a domain (i.e. /~ayoung48 to /).   The .htaccess file that ended up working properly is   RedirectMatch 301 \"/~ayoung48/(.*)\" \"https://Geekdude.github.io/$1\"   This redirect match command would correctly redirect pages from the old site to the new site. I will leave up this redirection until I lose access to the UTK servers.   Search Engine Optimization  Since the redirection was setup in the previous section, updating the search optimization is easier. I added the new site to Google Search Console and Microsoft Webmaster Tools. There did not seem to be an explicit place to let them know about the move1, but with redirection and having both sites listed under my account, I hope their web crawler will figure it out. I do not even need to redo the website verification method since it was already done for the old address and the verification files are still in place.   Moving Comments  Moving comments with Discus was easy as well. Since I have 301 redirection set up, I could just go into Admin -&gt; Moderate Comments -&gt; Tools -&gt; Migration Tools and use the Redirect Crawler. Then Discus ported the two comments I had on my old site to the new site. I also discovered that I am not getting email notifications when people comment. I will need to look into why. The settings are correctly set.                  There was one for google but it required Domain level property types, not URL Prefix like I am using. &#8617;           ","categories": ["Tech"],
        "tags": ["Website"],
        "url": "https://geekdude.github.io/tech/moving-website/",
        "teaser":"https://geekdude.github.io/assets/images/website-move-teaser.png"},{
        "title": "Getting started with Ctags with Vim and Git.",
        "excerpt":"Ctags with Vim are incredibly useful for quickly navigating around code. With them, Vim can jump between symbols to quickly go to the definition of symbols or find keywords within the document. Ctags + Vim end up working very similar to how Visual Studio Code or other IDEs “go to definition” and “find all symbols works.”   From the ctags website:     Ctags generates an index (or tag) file of language objects found in source files that allows these items to be quickly and easily located by a text editor or other utility. A tag signifies a language object for which an index entry is available (or, alternatively, the index entry created for that object).    Ctags with Vim   Ctags can be annoying to use because you have to run the ctags program to generate a tag file, which then needs to be found by Vim to be used. If the code changes, then the tag file will need to be re-updated. One way to set up ctags is to map a Vim keypress to generate the ctags. In my vimrc, I set it up to regenerate ctags when I press &lt;F5&gt; while not in a  file. More on the ctags command later.   \" Set f5 to generate tags for non-latex files augroup TexTags autocmd! TexTags autocmd FileType tex let b:latex=1 augroup end if !exists(\"b:latex\")     nnoremap &lt;f5&gt; :!ctags -R&lt;CR&gt; endif   In addition to generating the ctags, you also need to tell Vim where to find the generated tag files.   \" Ctags search set tags=./.tags;$HOME   This tags string says to search for the file .tags in the current file’s directory and recursively search upward until the user’s home directory is reached.   Ctags with Vim and Git   However, I do not manually generate the ctags file with &lt;F5&gt;. I store all my source code in Git, so instead of manually generating the ctags file, I use githooks to automatically generate the ctags files whenever I checkout, commit, merge, or rewrite. To setup githooks to be added automatically to newly cloned or created git repos, you make a git template with the correct files. The git template will be copied into the new repositories .git folder on creation.   First, setup the git_template structure.   ~/.git_template/ └── hooks     ├── ctags     ├── post-checkout     ├── post-commit     ├── post-merge     └── post-rewrite  1 directory, 5 files   ctags:  #!/bin/sh set -e dir=\"`git rev-parse --show-toplevel`\" trap 'rm -f \"$dir/.$$.tags\"' EXIT ctags -R --tag-relative --extra=+f -f\"$dir/.$$.tags\" --languages=-javascript,sql mv \"$dir/.$$.tags\" \"$dir/.tags\"   post-checkout:  #!/bin/sh dir=$(git rev-parse --git-dir) $dir/hooks/ctags &gt;/dev/null 2&gt;&amp;1 &amp;   post-commit:  #!/bin/sh dir=$(git rev-parse --git-dir) $dir/hooks/ctags &gt;/dev/null 2&gt;&amp;1 &amp;   post-merge:  #!/bin/sh dir=$(git rev-parse --git-dir) $dir/hooks/ctags &gt;/dev/null 2&gt;&amp;1 &amp;   post-rewrite:  #!/bin/sh dir=$(git rev-parse --git-dir) case \"$1\" in   rebase) exec $dir/hooks/post-merge ;; esac   You also have to add the template to your .gitconfig.  [init]     templatedir = ~/.git_template   Now more about the ctags command. To use the command, you first have to install ctags. I use exuberant-ctags which can be installed with sudo apt install exuberant-ctags on Ubuntu or Debian based Linux. The ctags command I use is   ctags -R --tag-relative --extra=+f -f\"$dir/.$$.tags\" --languages=-javascript,sql   The -R recurses into directories. The --tag-relative sets file paths relative to the tag file. The --extra=+f includes the entry for the base file name of every source file. The -f specifies that the tag file should be saved at the root of the git repository as .tags. Finally, the --languages removes javascript and SQL from the languages which get tagged.   As mentioned before, this template will only apply to new git repositories; therefore, I also created two fish functions to reload git hooks based on the git template. One file reloads hooks in a git repo without submodules, and the other one recursively updates the hooks of all submodules.   git-reload-hooks.fish:   function git-reload-hooks --description 'Reload git hooks' rm -f (git rev-parse --git-dir)/hooks/* git init end   git-reload-hooks-all.fish:   function git-reload-hooks-all --description 'Reload all git hooks'     git submodule foreach --recursive 'rm -f $(git rev-parse --git-dir)/hooks/*;git init' end   If all of this seems like a lot to setup, I recommend storing all your linux dotfiles in a git repository with a script to symlink the files to the right location. I plan to create a future post with more detail on how this is done.   References  I followed https://tbaggery.com/2011/08/08/effortless-ctags-with-git.html when I was setting up ctags for the first time. I have since modified my setup to work better with git submodules and other edge cases.   ","categories": ["Tech"],
        "tags": ["Git","Vim","Ctags"],
        "url": "https://geekdude.github.io/tech/ctags/",
        "teaser":"https://geekdude.github.io/assets/images/ctags-teaser.png"},{
        "title": "ASCR Workshop on Reimagining Codesign Whitepaper",
        "excerpt":"My whitepaper titled Emerging Heterogeneous Systems Provide Great Opportunities for Codesign for the ASCR Workshop on Reimagining Codesign was accepted.   All the accepted whitepapers can be found here.  ","categories": ["Research"],
        "tags": ["Heterogeneous Systems"],
        "url": "https://geekdude.github.io/research/ascr-reimagining-codesign/",
        "teaser":"https://geekdude.github.io/assets/images/ascr-reimagine-teaser.png"},{
        "title": "Happy Mother's Day",
        "excerpt":"Happy Mother’s Day! Below is a blender animation I made a few years ago after following the Blender Guru’s Donut Tutorial.            ","categories": ["Life"],
        "tags": ["Blender"],
        "url": "https://geekdude.github.io/life/happy-mothers-day/",
        "teaser":"https://geekdude.github.io/assets/images/blender-donut-teaser.png"},{
        "title": "Brother's Acting Demo Reel",
        "excerpt":"I helped my brother Preston edit together an acting demo reel using DaVinci Resolve. Check it out on his website at https://volweb.utk.edu/~pyoung15/acting/ or watch it below on Youtube.            ","categories": ["Life"],
        "tags": ["DaVinci Resolve"],
        "url": "https://geekdude.github.io/life/prestons-demo-reel/",
        "teaser":"https://geekdude.github.io/assets/images/demo-reel-teaser.png"}]
