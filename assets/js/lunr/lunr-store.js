var store = [{
        "title": "Engineering in England",
        "excerpt":"A Weekend in Cardiff, Wales—June 15–17 (2013)  by Josh Penney and Aaron Young   Originally Posted on tickle.utk.edu.   During the long weekend, we took a train out to Cardiff, which is the capital of Wales. We picked Cardiff for several reasons, but mostly for the Doctor Who Experience, which is a museum about the BBC show “Doctor Who” which happens to be one of our all-time favorite shows.   As we continued to explore Cardiff and the surrounding area throughout the weekend, we discovered some amazing castles dotted throughout the Welsh countryside. We visited three castles, Cardiff Castle, Caerphilly Castle, and Castel Coch.   Each castle had its own special feel ranging from the very nicely refurbished Castel Coch (shown below), to the much more impenetrable fortress of Caerphilly Castle.    Wales provided a very different cultural feel than we had experienced in the city and the countryside definitely operated at a much slower pace than downtown London.   It was a great trip to get away from the hustle and bustle of the city and explore the rich, and extremely green, Welsh countryside.                                                                                        ","categories": ["Life"],
        "tags": ["Travel"],
        "url": "/life/engineering-in-england/",
        "teaser": "/assets/images/engineering-in-england-castel-coch.jpg"
      },{
        "title": "New Website using Jekyll",
        "excerpt":"I decided that my project over Christmas break would be to create a new website. Although setting up the website overflowed past Christmas break, it is finally live. What motivated my decision was finally figuring out how to get my website to show up in a Google search. Spoiler alert, I registered my website with Google Search Console.   The Old Website  My old website was fine, but nothing special. A picture of the old site is shown below:  The old website consisted of a single page and its content was the same as the new About page. There were a few reasons I wanted to update my website.     I am not very good with HTML and I would rather create content for the website using markdown.   The previous website used no CSS and only had static HTML pages. It would be difficult to maintain navigation and constant style if I added additional webpages.   I thought it would be cool to try writing a technical blog, since my workflow has greatly improved from reading other people’s technical blogs.   I heard about GitHub pages and wanted to try creating a Jekyll website. Yes I know I’m not hosting this site on GitHub pages, but I will likely transfer my website there once I can no longer host my website on the University of Tennessee’s servers.   Why Jekyll?  So why did I go with Jekyll? For a few reasons. First, I wanted my website to be static, and I wanted a tool to help generate the static pages. Second, I learned about GitHub pages, which made me want to try out Jekyll so that I could possibly use GitHub pages for future project pages. And finally, Jekyll seems to have all the features I could want with great community support, which means that I would be able to find resources to help me to get started.   Learning Jekyll  I started learning Jekyll by reading the documentation provided on the Jekyll Website. I got the default template website running with the quick start guide and then worked my way through the step by step tutorial. The tutorial was well put together, but a little overwhelming since it is written for someone with web developing experience. It did help me figure out roughly how Jekyll generates the web pages, and made me realize that I wanted to use an existing theme instead of trying to build my own. The other main documentation for Jekyll is well put together and easy to navigate once the terminology is understood.   Because it was hard for me to see how to use Jekyll for my project based on the documentation, I also read other people’s blog posts which detailed how they went about creating their Jekyll websites. The two that I found most helpful are Create your Academic Website in Jekyll and Host it on GitHub by Steven Miller and Build your own website (with Jekyll and Minimal-mistakes theme) by Li Zeng. You might notice that my website looks very similar to Li Zeng’s and that is because I also decided to go with the Minimal Mistakes theme.   Finding a Template/Theme  When I was deciding how I wanted my website to look and be structured, I searched the web to try to find examples of personal websites that I could use for inspiration. Many of the sites I found were primarily used as portfolios and therefore showcased the work or the writings of the person. I knew that my website only needed a landing page, about page, blog (If I thought I would create posts), and a link to my resume. The key to the content I wanted is that it would consist of multiple pages and would be set up as a personal academic website. This ruled out all the themes which focused on a single landing page or didn’t fit the personal website aesthetic.   When searching for a theme, I looked through jekyllthemes.org and jekyllthemes.io. I preferred jekyllthemes.org since all the themes are free, however, some of the themes have bad links. Jekyllthemes.io was also good to look through and you can filter the results by free websites and all the themes had good links. Many of the better looking themes can be found on both sites. I found 7 themes that caught my attention as potential candidates. Then I narrowed the search down to two favorites. The final two were Minimal Mistakes and indigo. I ended up choosing Minimal Mistakes, since I liked the more content heavy splash page layout for a homepage, and the theme seemed feature rich and well-designed. It got bonus points for the well laid out documentation that was written using the theme. Indigo was close second since it is also well-designed and a beautiful minimal design with a very clean landing page.   Although there are differences in the parameters used between themes, the content for the site is all in markdown. So with some time invested in configuring a new theme, I could migrate my site to a different theme. The downside is that, since the themes have different configurations and layouts, different themes are not interchangeable without some work.   Making Content  The first step I took was to read the documentation for Minimal Mistakes. The documentation is structured well and custom configuring the site was easy and fun. I then ported over my old website’s content to make the about page. I then spent some time learning how to embed the resume document into the webpage. Next, I structured the home page and selected graphics to use. I found two useful resources for images. Font Awesome has a wide range of free SVG icons. I used their envelope icon for my contact me graphic. The second resource is Unsplash which has free high resolution photos. Unsplash is where I got the typewriter image I used for the resume graphic. That’s about it, the rest of the website setup was done following the Minimal Mistakes documentation.   Tada Done  So now I have a new website. Thank you for visiting and reading about it. If you have any questions or comments about the new website, please don’t hesitate to leave a comment below.  ","categories": ["Tech"],
        "tags": ["Website"],
        "url": "/tech/new-website/",
        "teaser": "/assets/images/old_website.png"
      },{
        "title": "Challenges with New Website Deployment (Jekyll and Subdomains)",
        "excerpt":"When I was deploying my website to the web server, I ran into a few challenges.  I will talk about them here in case anyone happens to run into similar  challenges.   SVG Graphical Issues  One issue that stood out was how chrome was rendering my SVG graphics. I  created the graphics with Inkscape and when they were displayed through Chrome,  some of the graphics had noticeable issues. The main problem was unsupported  fonts and Inkscape specific SVG features. I assumed the problems would only be  worse if I tested with multiple browsers, so to fix the issue, I converted all  of my SVG graphics to PNG images. I figured that the PNG images would be able  to be rendered properly regardless of the browser. Although this means that the  quality of the vector graphics is reduced, I think it is still a better  solution than having graphics that look incorrect. I could take the effort make  sure I only use standard SVG elements, that any viewer should recognize, but  that seemed like too much work to get right when PNG images already work well  enough.   Deploying to a Subdomain of a Site  The biggest issue came from the fact that I deployed the website to a subdomain  of the site (i.e. my site lives at http://web.eecs.utk.edu/~ayoung48/ and not  http://web.eecs.utk.edu/). This resulted in all of my assets referenced from  markdown pages resulting in broken links, as the browser would try to find the  asset at the root of the site and not at the root of the subdomain  /~ayoung48/. However, I couldn’t just append the subdomain to each of the  hyperlinks, as doing so would break the ability to test the site locally.  Luckily Jekyll already has a solution for this problem. Since the situation of  deploying to a subdomain is less common, it was harder to find good information  explaining the solution.   The first website I used to fix the issue was the Minimal Mistakes  Configuration Documentation. The configuration setting I needed to set was the site-base-url. This setting  specifies the subdomain of the site the website is deployed to. In my case, the  site-base-url is /~ayoung48. With this configuration change, the correct  subdomain is used when I serve the website locally for testing.  Parker  provides a brief explanation into base url, if you would like further  clarification. This change still didn’t fix the links, but resolved the local  deployment testing.   To fix the links, I had help from the liquid  filters and liquid  tags documentation pages. To  summarize, there are multiple was to have liquid fill in the correct url (with  the baseurl) for you. I’m going to explain each method using the old website  image found in the previous post. One method is to use the prepend filter:   ![Old Website]({{ '/assets/images/old_website.png' | prepend:site.baseurl }})  This method is OK, it just prepends the string found in site.baseurl to the  url, but there are better ways. Another way is to use the relative_url or  absolute_url filters.  ![Old Website]({{ '/assets/images/old_website.png' | relative_url }}) ![Old Website]({{ '/assets/images/old_website.png' | absolute_url }})  With these filters relative_url will generate  /~ayoung48/assets/images/old_website.png  and absolute_url will generate  https://web.eecs.utk.edu/~ayoung48/assets/images/old_website.png  I like these filters better than the prepend filter, since they give a clearer  intent on what you are trying to accomplish.   The last way I found to generate the links is with the link tag.  ![Old Website]({{ site.baseurl }}{% link /assets/images/old_website.png %})  Although this syntax is the worst looking, it gives the added benefit of  correctly generating the permalink of the file you are trying to link to. This  means that it is the preferred method for linking to other generated pages,  such as posts, since the permalink generation style could change. With this  method the link will still work correctly if the permalink style is changed.  This method also has the added benefit of performing link validation when  Jekyll builds the site. If you use the link or post_url tags, Jekyll will  check to make sure the link or post exists during the build process and throw  an error if the link is bad. Since tags provide correct generation of  permalinks and perform link validation, they are my preferred method for  specifying local links in my website.   Width Issue of Facebook Comments  Another issue I ran into was that on one computer, for whatever reason, the  Facebook comments would not take up the correct width. The fix for this problem  was to copy some css code I found on the internet to /assets/css/main.scss.  This solution was found on  stackoverflow.  // Fix width issues of Facebook comments. // https://stackoverflow.com/questions/22243233/how-to-make-facebook-comment-box-width-100-2014 .fb_iframe_widget_fluid_desktop, .fb_iframe_widget_fluid_desktop span, .fb_iframe_widget_fluid_desktop iframe {             max-width: 100% !important;             width: 100% !important;  }   Deploy Script  This last section isn’t really a deployment issue, but a way to make deploying  the website easier. I created a simple script to build the Jekyll site and copy  the files to the server.  #!/bin/bash #------------------------------------------------------------------------------- # Script to deploy the website to the server. #-------------------------------------------------------------------------------  # Variables -- Change to the correct values before use. instancehost=\"machine_address\" remotewebroot=\"webroot_folder\"  # Build Jekyll Site JEKYLL_ENV=production bundle exec jekyll build chmod -R a+rX _site  echo \"rsync to SSH host $instancehost ...\"  # Long form of rsync options # rsync --verbose --recursive --compress --checksum --human-readable --perms --delete-after` rsync -vrzchp --delete-after _site/ $instancehost:$remotewebroot   Update on 1/14/2020 for Jekyll 4.0  With the release of Jekyll 4.0, the link and post_url tags no longer need site.baseurl prepended every time they are used. These tags now use their relative_url filter to correctly take care of prepending the site.baseurl. This does mean that existing uses of the prepend pattern will break and the site.baseurl part should be removed. I like this change since it makes linking shorter, but a site wide find and replace was needed to fix the links.   So in summary, the recommended was to generate links is with the link tag. For example:  ![Old Website]({% link /assets/images/old_website.png %})  This method is now the most concise, readable, and will correctly generate the permalink URL following the site rules. It will also perform link validation to make sure the linked file exists.   ","categories": ["Tech"],
        "tags": ["Website"],
        "url": "/tech/new-website-challenges/",
        "teaser": "/assets/images/old_website.png"
      },{
        "title": "Skip Barber 3-Day Racing School",
        "excerpt":"Recently my Dad and I attended a Skip Barber 3-Day Racing  School at Road  Atlanta in Georgia. The trip was a blast and driving on the track in a Mustang  GT was exciting. Before we  left to go on the trip, I practiced in my Dad’s simulator to get a good feel  for the racing line around the track. I had previously done a lead follow at  Road Atlanta in street cars, but this was my first time driving a race car, and  my first time to drive around a track on my own. The cars were setup as race  cars. They had a racing air intake, a racing transmission, racing brakes and  suspension, and they had a roll cage with a five point harness. The car I used  was number 18.      At the racing school they taught us everything we would need to know to  understand car dynamics and to drive around a track safely and quickly. Each  day was broken up into two halves, before and after lunch. Each half then  consisted of in-classroom learning followed by a change to put what we learned  to test-driving the car. During the morning session of the first day we learned  about car mass, tire load, and various types of oversteer and understeer, and  the various causes of each. We also learned how to use our eyes to look where  we want to go and look further down the track to plan the next move while the  current one is being executed. The classroom lecture was quite entertaining and  the instructor intermingled the lesson with various stories. After the  classroom lecture, we were split up into two groups, red and blue. Each group  had 5 people in it. They red group first went to the autocross track and  practiced using our eyes to drive the car smoothly around the track. Cones  where setup for the turn-in, apex, and turn-out for each of the turns. The  instructors rode along with us and gave us advice as we navigated the autocross  course. After that, we went to the skid pad and learned what oversteer and  understeer feel like. We also practiced CPR (Correct, Pause, Recover) to  prevent the car from spinning out of control when oversteer happens. I was  actually fairly good at recovering the car, and sliding the car around was a  blast. One of the instructors had me try to drift the car under power once I  had succeeded in recovering the slides without throttle. Balancing the throttle  to continue the slide made recovery much harder.   During the afternoon session day 1, we learned how to downshift with the  toe-heel technique. We also discussed the racing line more and covered how to  take various turns. Then we left the classroom and practiced downshifting with  an exercise they had set up in the pit area. We also rode around the track in a  van to learn the racing line. We had to buckle up in the van since they took it  around the track at a good pace. After being show the track in the van, the two  groups took turns driving in lead follow sessions with a pace car and riding in  the pace car. After the first session the instructors stopped riding in the  race cars and all the driving was done solo. The lead follows were fun and  riding in the pace cars gave you a chance to hear commentary from the  instructor and see the race line further. The last turn I took in the pace car,  the instructor was really pushing the rental car. He was squealing the tires  and taking all the curbs. He knocked over some cones marking the turn apex.         The second day, in the morning, the lecture covered more information on grip,  turns, and flags. On the track, we did more lead follow and also took a turn  driving the race line in the rental car with the instructor and other students  riding along. After this, we started driving around the track solo with a stop  box setup. We would drive around the track and then stop on the back straight  to get feedback over the radio on how we did on the turns and if there is  anything to focus on improving the next lap. Initially there was a low speed  limit and rpm limit, but as we continued driving the speed limit and rpm limit  were slowly relaxed.   In the afternoon of the second day, we discussed braking and the friction  circle. After which, we went to the track and did braking exercises at turn 10.  With braking you put strong initial force on the brake and then slowly release  the force. ABS should engage or be just before engaging as your are braking.  Then during the turn you have a slight amount of brake still applied which is  known as trailing brake. If done correctly, your keep the tires at maximum  traction throughout the entire turn. After the exercise, we continued to do  stop box lapping sessions. When it was the other groups turn to drive, we would  travel to one of the flag stations along the track and watch the other group  drive. It was fun to see different views of the track and to critique the other  group to learn how we could better drive when it was our turn again.      The last day, in the morning, the lecture covered passing technique and driving  in the rain. On the track, we drove a passing exercise at turn 10, where we  drove off of the normal line to complete a pass. We then proceeded to do more  stop box lapping sessions with the speed increased.   In the afternoon, we learned how to conduct race starts. On the track, we  practiced a real race start. We did three starts, two double file starts and  one single file start, then transitioned into an open lapping session. The race  starts where intimidating since you where surrounded by other cars, but the  exercise itself wasn’t that bad. You raced from when the green flag was thrown  on the front straight away and stopped racing at the first turn. The open lap  sessions are when the stop box is taken down, and you drive around the track  until the checkered flag is shown. Passing is done by point by. If you see a  car behind you, your point them by on one of the straightaways. All that means  is your point which side of the car you want them to pass your on and then you  slow up a little, so they can complete the pass. After we finished driving our  last open lapping session, we were taken on a hot lap by one of the  instructors. And let me tell you that the instructors went much faster than I  was able to. I enjoyed going a comfortable pace around the track. After the hot  lapping sessions, we went back to the classroom and had a graduation ceremony.  I now have a diploma from the racing school, and I am eligible to apply for a  SCCA racing license. More importantly, I can now participate in driving on the  track for the One Lap of America driving  competition I am doing with my Dad later in the spring.     ","categories": ["Life"],
        "tags": ["Driving"],
        "url": "/life/skip-barber-racing-school/",
        "teaser": "/assets/images/road-atlanta-with-car.jpg"
      },{
        "title": "One Lap of America 2019",
        "excerpt":"Recently my dad and I competed in Brock Yates’ One Lap of America driving  competition. The official website for the event is found at  onelapofamerica.com. Since the event was  technically a competition, the events were time trials. Each team was scored  and ranked based on how fast they completed each event. The results and  rankings from the event can be found on the results  page. The car we used for the event is my dad’s 2017 Chevrolet Stingray. This put us  in the SGT-1 BB and Stock - GT Classes, where we placed 17th and  15th respectively. Our overall standing was 63rd.   Our goal, however, was not to place well, but to have fun, complete the event,  and bring the car back home, and we completed all of these goals. The  competition was long, but fun. Each day had roughly the same layout. We would  wake up early, around 7, then head to the track, where we would drive in two  events. After which, we would then drive for around 500 miles to reach the next  hotel, so that we could do it all again. The complete schedule for the event is  show below.                                                  Day 0 Travel  The first day we packed up the car and traveled to the first hotel. We knew we were at the right place when we saw a viper with one lap stickers  and a Lamborghini next to it.                                                                                          Day 0 Gallery.       Day 1 Registration  The first day of the event we added stickers to our car, went through tech  inspection, walked around and looked at all the other cars, received our  information book, and attended a drivers meeting.                                                                                                                                Day 1 Gallery.       Day 2 Skid Pad and Autocross, South Bend and Kokomo, Indiana  The first day of driving, we started off at the Tire Rack Headquarters where we  drove on a wet skid pad. The goal was to see how much G-force you could put  into turning.   After the skid pad, we then went to Grissom Air Force Base where my dad drove  the autocross event. Before the event we both walked the course many times and  discussed strategies on how to drive it.   We closed off the day by driving from Indiana to our hotel in Ohio.                                                                                          Day 2 Gallery.                   Day 3 Nelson Ledges, Garrettsville, Ohio  The second day of driving, we were at Nelson Ledges Road course. We got their  early enough that we were able to walk around the course before the track went  live. There was an old bridge that went over the starting line. The bridge  would shake when cars passed under it. My dad drove the course, but there were  parade laps after lunch that I drove.                                                                                                                                                                      Day 3 Gallery.                   Day 4 Road America, Plymouth, Wisconsin  The next track was Road America. In the morning there was rain and the track  was very slippery. There were some wreaks as cars slid off of the track, so our  run group at the end of the pack was delayed until after lunch. Going after  lunch had the great benefit of allowing the track to dry and the sun to come  up. This track was very large and there were many walking paths around the  track and places to spectate.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Day 4 Gallery.                   Day 5 Brainerd International Raceway, Brainerd, Minnesota  Day 5 was one of the nicest, sunniest, and longest days of the trip. In the  morning I started to walk the course, but I only made it down the drag strip  portion of the front straight before I realized just how long the total course  would be. This track also had great places to spectate. The morning event was  the large full course and I spectated the driving from the top of the sky box  building. The track had a tunnel under the straight to cross from one side of  the track to the other.   In the afternoon, we drove on the shorter SCCA road course portion of the  track. This section of the track also had a rooftop to spectate from. The  shorter course was fun to watch, since you could see for a greater percentage  of the track and close to the start/finish line were fun turns to watch.  Travis Pastrana was fun to  watch go around one of the turns in particular. He would power slide each time  he went around. People spectating closer said that he was waving to the crowd  of people watching while doing the power slide. In the morning run, he was  talking on the phone with his teammates in the stands as he was driving around  the track. It was great fun to hear his conversations/interviews and watch him  drive.   Next we did drag racing on the front straight. The first time on the drag  strip, you tried to see how fast you could go. Then they did a bracket race,  where cars would try to get as close to their first time as possible without  going over their previous time. In the bracket, they would start the cars at  different times so that they should arrive at the finish line at the same time  (assuming they ran the same time as before).  Finally to finish off the day, we  had one of the longest transit drives of the trip, 555.5 miles.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Day 5 Gallery.                   Day 6 Motorsports Park Hastings, Hastings, Nebraska  Hastings was a smaller course. We were able to walk the course in the morning.  The weather was rainy and windy, but there were shelters from the rain. They  had multiple covers and also a party bus. The party bus was nice since in  addition to blocking the rain, it would block the wind. Even with a fire suit  and multiple layers, it was cold this far north. This is also where our EZ pop  up canopy finally collapsed. It was starting to look rough, and had already  blown over once. This time we strapped it down really well with bags and  hammock straps, and instead of blowing away, it folded in on itself. However,  our stuff still stayed dry and we ended up not needing it for the rest of the  trip. This place also had the best food. They had only one thing on the menu  and it was a delicious brisket sandwich.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Day 6 Gallery.                   Day 7 Blackhawk Farms, South Beloit, Illinois  Blackhawk Farms was a nice course. By now we had the rhythm down. Wake up, get  coffee, drive to course, walk course, morning session, lunch, afternoon  session, transit drive, Cliff Bars and gas station pizza for supper, check-in  and collapse in hotel bed. Repeat. By this point a nap was required, and I was  glad to have brought a hammock to nap in. This track has a second story  building to watch from, but the best place to watch was at some metal stands in  the U portion of the track.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Day 7 Gallery.                   Day 8 National Corvette Museum (NCM) Motorsports Park, Bowling Green, Kentucky  I drove the events at NCM and it was a blast. The weekend before I went to a  Chin Track Days here, so I already knew the course and had multiple sessions of  experience driving it. In the morning, we first drove the east course, then  afterwards we moved directly into line to drive the west course. Driving the  split up course was new, but pit-in and pit-out was easy to find, and we looked  for them when we walked on the course in the morning. I was able to run good  times and I felt more confident on the track than I did during the week before.  The previous experience helped tremendously. The sink hole portion was the most  fun, and I was able to squeak the tires for most of the entrance turn. My time  on the west course was comparably better, but the straights were to the  advantage of our car. For lunch they had hibachi from a food truck. I was happy  to have Asian after all of the BBQ I had been eating earlier in the week.   After lunch we drove on the full course. I enjoyed the full course and I was  back in my comfort zone driving the same configuration I had during Chin. The  laps were good and the only sad part was I forgot to hit record on the car’s  race cam, so I don’t have any footage from that run. And without any proof to  the contrary, those were my best laps on the track ☺.   After the event, there was still time left for the track rental, so they opened  up the track for open lapping. My dad was finally able to drive on the track at  speed. Previously he had only done a parade lap on the course. This was the  last track day, but we still had to drive back up to Indiana for the skid pad  part 2, banquet, and group photo. So after being in Kentucky (closest  destination to home) we heading back north for the last day.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Day 8 Gallery.                   Day 9 Skid Pad, South Bend, Indiana   First we line up all the cars to take a group photo. Then I got a chance to  drive the skid pad again, but this time on a dry track. I did much better in  the dry and I had more confidence to go fast. I think I was much closer to the  limit of the tires this time around and the car was sliding around more. For  this event they ran the cars in reverse order in the standings, and even so, it  was great to hear that I took first place on the skid pad after completing my  run. Finally the event ended with an awards banquet where the food was good and  I learned what a shoey  was.   And so after the banquet our trip was complete. We had successfully made it  around One Lap of America. Tired but happy, we then made our last transit drive  back home. At this point we were pros at long drives and we made it home  safely.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Day 9 Gallery.                   ","categories": ["Life"],
        "tags": ["Driving"],
        "url": "/life/one-lap-of-america/",
        "teaser": "/assets/images/one-lap-day-1-left.jpg"
      },{
        "title": "TENNLab---Neuromorphic Architectures, Learning, Applications",
        "excerpt":"I am part of the TENNLab—Neuromorphic Architectures, Learning, Applications research group at the University of Tennessee. We are researching a new paradigm of computing, inspired by the human brain. Our research encompasses nearly every facet of the neuromorphic area, including current and emergent hardware implementations, theoretical models, programming techniques and applications. Specifically, I have worked on framework development, the DANNA and DANNA2 digital neuromorphic processors, and neuromorphic application development. I have also helped with the development of multiple neuromorphic robots, including NeoN (Neuromorphic Control System for Autonomous Robotic Navigation), GRANT (Ground-Roaming Autonomous Neuromorphic Targeter), and SABR (Self-Adjusting Balancing Robot). My focus has been on communication between traditional computers and neuromorphic processors. I have developed the NACC (Neuromorphic Array Communications Controller) to support high-speed, low-latency communication between the host PC and the neuromorphic system. I have also designed and built SNACC (Scaled-up Neuromorphic Array Communications Controller), which uses a NACC to connect multiple neuromorphic processors together for the purpose of building a large unified neuromorphic system. This system is designed to run large neural networks and also supports real-time communication with the Host PC.   SNACC     NACC     DANNA2 Viz    ","categories": ["Research"],
        "tags": ["Neuromorphic Computing"],
        "url": "/research/tennlab/",
        "teaser": "/assets/images/tennlab-poster.png"
      },{
        "title": "A Review of Spiking Neuromorphic Hardware Communication Systems",
        "excerpt":"Aaron R. Young, Mark E. Dean, James S. Plank, and Garrett S. Rose   September, 2019   IEEE Access   https://ieeexplore.ieee.org/document/8843969   View Article   Abstract  Multiple neuromorphic systems use spiking neural networks (SNNs) to perform computation in a way that is inspired by concepts learned about the human brain. SNNs are artificial networks made up of neurons that fire a pulse, or spike, once the accumulated value of the inputs to the neuron exceeds a threshold. One of the most challenging parts of designing neuromorphic hardware is handling the vast degree of connectivity that neurons have with each other in the form of synaptic connections. This paper analyzes the neuromorphic systems Neurogrid, Braindrop, SpiNNaker, BrainScaleS, TrueNorth, Loihi, Darwin, and Dynap-SEL; and discusses the design of large scale spiking communication networks used in such systems. In particular, this paper looks at how each of these systems solved the challenges of forming packets with spiking information and how these packets are routed within the system. The routing of packets is analyzed at two scales: How the packets should be routed when traveling a short distance, and how the packets should be routed over longer global connections. Additional topics, such as the use of asynchronous circuits, robustness in communication, connection with a host machine, and network synchronization are also covered.          Graphical summary of neuromorphic hardware communication systems. The top half of this figure summarizes the routing schemes used by the neuromorphic systems, and the bottom half summarizes the routing methods.        Citation Information  Text   author    A. R. Young and M. E. Dean and J. S. Plank and G. S. Rose title     A Review of Spiking Neuromorphic Hardware Communication Systems journal   IEEE Access volume    7 pages     135606-135620 year      2019 doi       10.1109/ACCESS.2019.2941772 url       http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8843969&amp;isnumber=8600701   BibTex   @ARTICLE{ydp:19:ars,     author = \"A. R. Young and M. E. Dean and J. S. Plank and G. S. Rose\",     title = \"A Review of Spiking Neuromorphic Hardware Communication Systems\",     journal = \"IEEE Access\",     volume = \"7\",     pages = \"135606-135620\",     year = \"2019\",     doi = \"10.1109/ACCESS.2019.2941772\",     url = \"http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8843969&amp;isnumber=8600701\" }   ","categories": ["Research"],
        "tags": ["Neuromorphic Computing"],
        "url": "/research/ieee-access-2019-review-paper/",
        "teaser": "/assets/images/ieee-access-2019-graphical-abstract.png"
      },{
        "title": "The Big Orange Bramble",
        "excerpt":"This year marks the three-year anniversary of the 64 node Raspberry Pi cluster known as the Big Orange Bramble or BOB. I am happy to report that BOB is still running strong and there have been no major downtimes or issues with the system. BOB is still occasionally used to train neuromorphic networks using EONS, which you can read more about here. BOB was build as part of the special topics class ECE599 Supercomputer Design and Analysis, Summer 2016 of which I was part. This post includes images of the system and copies of the reports and presentations in case you want to read more about the system. The reports are also posted on Dr. Mark Dean’s website. Dr. Dean taught the course. Also included are the reports for the follow-up project named A Linux Integrated Computing Environment or ALICE, which used Pine64 boards and Nvidia TX1 GPUs.   Abstract   This project involved the design and construction of a high performance cluster composed of 68 quad-core ARMv8 64-bit Raspberry Pi 3s. The primary intent of the project was to establish the operating environment, communication structure, application frameworks, application development tools, and libraries necessary to support the effective operation of a high performance computer model for the students and faculty in the Electrical Engineering and Computer Science Department of the University of Tennessee to utilize. As a foundation, the system borrowed heavily from the Tiny Titan system constructed by the Oak Ridge National Laboratory, which was a similar but smaller-scale project consisting of 9 first generation Raspberry Pis. Beyond the primary target of delivering a functional system, efforts were focused on application development, performance benchmarking, and delivery of a comprehensive build/usage guide to aid those who wish to build upon the efforts of this project.   Picture Gallery                                                                                                                                                                                                                                                                                                                              BOB Gallery.       Project Reports  BOB Documentation   Applications for 68 Node Raspberry Pi 3 Education Cluster   Performance, Management, and Monitoring of 68 Node Raspberry Pi 3 Education Cluster: Big Orange Bramble (BOB)   ALICE Documentation   Project Presentations  Slides: BOB Presentation   Video: Big Orange Bramble (BOB) HW SW Systems Presentations   Video: Big Orange Bramble (BOB) App Presentations   Slides: ALICE Presentation  ","categories": ["Tech"],
        "tags": ["High Performance Computing"],
        "url": "/tech/bob/",
        "teaser": "/assets/images/bob-hardware-diagram.png"
      },{
        "title": "Reorder",
        "excerpt":"I recently created a reorder python script to rename, reorder, add, and remove files/directories in a directory with a numerical naming scheme.   A common file naming practice is to prepend a number to the filename so that the files appear in the order specified by the number. However, say you want to add, delete, or reorder these files, you would have to go through and update all the filenames which have numbers changed from the operation. Although renaming the files like this is not hard, it’s annoying to do when there are multiple files which need to be renamed. I ran into this issue recently while writing my dissertation; each chapter is broken into its own .tex file stored in the chapters/ directory.   chapters ├── 01-introduction.tex ├── 02-related-work.tex ├── 03-previous-work.tex ├── 04-goals.tex ├── 05-tools.tex ├── 06-danna2-work.tex ├── 07-snacc.tex ├── 08-challenges.tex ├── 09-applications-performance.tex ├── 10-accomplishments.tex ├── 11-future.tex └── 12-conclusion.tex   In order to insert a new chapter or delete a chapter, all the files which come after that chapter will have to be renamed. Further, more in the main .tex file I include each of the chapters with an include statement.   \\include{chapters/01-introduction} \\include{chapters/02-related-work} \\include{chapters/03-previous-work} \\include{chapters/04-goals} \\include{chapters/05-tools} \\include{chapters/06-danna2-work} \\include{chapters/07-snacc} \\include{chapters/08-challenges} \\include{chapters/09-applications-performance} \\include{chapters/10-accomplishments} \\include{chapters/11-future} \\include{chapters/12-conclusion}   This means I have to renumber the files in the directory and the files in the include statements. In order to make this operation easier, I created a reorder python script to easily rename, add, delete, and reorder the *.tex files in the chapters directory. This script opens up a text buffer, and you make the desired changes to the buffer; then the script will make the changes to the directory and update the file numbers.   Additionally, I modified my makefile for the \\(\\LaTeX\\) project to include a target to build a chapters.tex file with the proper include statements for the chapters found in the chapters folder. Now in main.tex I only need to \\input{chapters.tex}.   Now if I want to add a new chapter between previous-work and goals, as well as rename “previous work” to “prior work”, all I need to do is run reorder chapters. I am then presented with the buffer   01-introduction.tex 02-related-work.tex 03-previous-work.tex 04-goals.tex 05-tools.tex 06-danna2-work.tex 07-snacc.tex 08-challenges.tex 09-applications-performance.tex 10-accomplishments.tex 11-future.tex 12-conclusion.tex   I then change the buffer to   01-introduction.tex 02-related-work.tex 03-prior-work.tex new-chapter 04-goals.tex 05-tools.tex 06-danna2-work.tex 07-snacc.tex 08-challenges.tex 09-applications-performance.tex 10-accomplishments.tex 11-future.tex 12-conclusion.tex   Then I can rebuild the \\(\\LaTeX\\) document and the changes to the chapters will be included with a newly generated chapters.tex.   Much appreciation to Jonathan Ambrose who helped come up with the idea for the script, motivated me to write it, and added several additional features to the script.  ","categories": ["Tech"],
        "tags": ["File-Organization"],
        "url": "/tech/reorder/",
        "teaser": "/assets/images/reorder-teaser.png"
      },{
        "title": "Sourdough Bread",
        "excerpt":"My favorite type of bread is a fresh, warm loaf of sourdough. I can almost go through a whole loaf by myself when it is fresh from the oven. Sourdough bread also makes the best french toast and great bread crumbs for wiener schnitzel. This post will go over the basics to getting started making sourdough bread. It includes links to the resources I used to get started, my favorite recipes, and some tips and tricks I have learned over my years of baking and experimenting for fun.   Starter  At first I tried to make my own starter. This didn’t really work out all that well. I learned that you should not try to make a sourdough starter by initially adding in baking yeast. If you do this, you will not get the good sourdough flavor and it will be harder to form the symbiotic relationship between bacteria and wild yeast which gives sourdough its unique acidic flavor. After a first bland attempt and subsequent difficulty to get a starter to grow with wild yeast (i.e. with only flour and water). I decided to buy starter from King Arthur Flour. This starter worked much better. I bought it in 2013 and I have kept it going ever since. I keep the sourdough starter in a King Arthur Sourdough Crock. They also sale the starter in a set with the crock.   Starter Care  King Arthur Flour provides a great sourdough guide to learn how to get started with sourdough bread. It includes information on how to maintain a starter as well as how to bake bread. This is where I first started and I still use much of what I learned here.   I generally end up neglecting my starter for a few months at a time when I get busy and didn’t have time to bake. The starter will do well for this long in the fridge. Do not freeze the starter since doing so could harm the microbes. If I’m not planing on using my starer within the next few days, I will keep it in the fridge. The cold slows down the activity of the organisms which also slows down the rate they consume the flour. After a while, brown liquid will form on the top of the starter. This is expected and can be mixed back into the starter before feeding. If the starter ever shows signs of mold, has an orange or pink tint/streak or smells putrid then it went bad with harmful microorganism and you will want to throw it out. I don’t know how common this is as mine has never spoiled. The smell of sourdough is quite sharp but it should smell like beer or bread and not like spoiled food.   When I feed my starter I mix together a cup of starter, a cup of water, and a cup of flour. I usually use a scale which means I mix together 172 grams of starter, water, and flour. You can vary the ratios, but I found that the same amount of each works well. I aim for a pancake batter like consistency. Too much water and there isn’t as much food. Too much flour and the starter can expand outside the container and makes a mess. A nice balance will bubble, but not trap the air so that it expands.   There is a difference between a fed and unfed starter. A fed starer is active and bubbling. Typically, you can feed the starter and wait a few hours and it will be active. An unfed starter is straight from the fridge before feeding. If it has been a long time since I fed the starter, I will feed it, let it sit at room temperature for a day, feed it again, and wait for it to appear active before I consider it fed. I have made bread with both fed and unfed starter. The fed starter makes it easier to get fluffier bread but unfed still works.   Another great trick I learned is that you can easily transfer starter by taking some of it and adding lots of flour so that it forms a dry ball. The dry ball can then be stored in the top of a bag of flour you are taking on a trip. I have used this to transfer my starter when I traveled to visit relatives and I wanted to make sourdough bread for them. The bag of flour with dried starter in the top was much easier to transfer then the ceramic crock.   Sourdough Bread  Bread making is as much an art as a science. There are many factors at work to determine how the final loaf will turn out. These variables include the activity of the starter, the water-to-flour ratio, and the time and temperatures for the various fermenting, proofing, and rising stages. Although I started out following the Extra-Tangy Sourdough Bread recipe, I also started experimenting with this recipe to learn how different ingredients and techniques affected the final loaf. Now I still loosely follow the recipe, but I do it more from experience and practice then from precisely measuring everything. My goal was actually to simplify the process as much as possible, down to the key ingredients, understand the role these ingredients play and then understand the phases of the proofing process. I found that the bread mostly came out well every time and the variation was fun. My current bread relies on simple ingredients and gets the great sourdough flavor from only the starter and the proofing steps.   My approach to making bread has been shaped by reading a variety of books, reading internet posts, and from watching YouTube videos. Of particular note is In Celebration of Simplicity: The Joy of Living Lightly which is about living a life based on gospel simplicity through an extended metaphor with the simple ingredients in bread. This book inspired me to keep the ingredients simple and to understand the role of each. The bread chapter in Cooked: A Natural History of Transformation was very interesting and led me to experimenting with whole wheat sourdough bread and also to use wetter dough. The whole wheat was very challenging, but I could eventually make a good sandwich bread loaf. However, I could never get as yummy of an artesian loaf as I could with white flour. The introduction to Classic Sourdoughs has a great, concise, introduction to the art of sourdough bread making with information on the various factors and how they affect the final loaf.   My Method of Baking Bread  So I will now walk you through my process making bread, but for a recipe and more details instructions follow the Extra-Tangy Sourdough Bread recipe and start experimenting. I start off the day before by mixing together a small mason jar of water (2-ish cups), the leftover starter after feeding it, and a sifter full of flour (3-ish cups) in a large glass mixing bowl. I spend a while mixing this together until it is quite smooth 1-3 minutes. At this stage it should be uniform and still liquidy like pancake batter. You want it to be liquidy so that it won’t overflow the bowl overnight. Then let the bowl sit out on the counter until you go to bed, then move it into the fridge (I will leave it out all night if I used an unfed starter).   The next day I just add some salt, honey, and herbs. With these ingreadients, I stopped measuring the amounts percisely. Start with the amounts listed in the recipe, but after awhile you get a feel for the rough amounts and no longer have to measure precisely. I like using thyme or a mix of Italian herbs. For this part I take inspiration from Penelope Wilcock who says in In Celebration of Simplicity: The Joy of Living Lightly “When I add salt, yeast, honey and herbs into my bread, I just put in ‘some’. Approximate amounts will do, creating acceptable variants in the taste.” I no longer add sour salt to my bread, since I am able to get a strong natural sourdough flavor without it. At this point I also usually add in some olive oil. If you want artesian bread, then leave the olive oil out. But if you want sandwich bread with a softer crust include it. The oil will reduce the size of the air pockets and make the crust softer and better for sandwich bread. Again with the oil, I don’t measure precisely but pour in ‘some’ from my tin olive oil container. I probably add around a tablespoon or two. On the amount of oil to add, Penelope says “I put lots in.” I mix everything together, so that it is uniformly combined. Then I either dump it out onto a bed of flour or start adding flour while I am stirring. I mix the added flour in with the dough.   At this point I should probably let the dough rest for a bit to autolyse but normally I forget and just start with the kneading. While I knead I try to keep the dough as wet as I want to deal with, as I start to knead it using the “slap” kneading method. This method is shown in the King Arthur Kneading video and requires less effort than other methods and allows stickier dough to be handled. This main kneading needs to be long and I do it for at least 10 minutes. As you continue kneading the dough will start to get firmer and harder to knead. This means you are closer to finishing. I try to keep it sticky and will occasionally add flour to the outside to make it less sticky. While I am kneading it, I add more flour based on the constency of the dough. From Cooked: A Natural History of Transformation, the wetter the dough, the better for artisan breads. However, I like to find my happy middle ground with a sticky dough, but one that is dry enough to no lose its structure during the proofing process. With really wet dough, you should fold the dough as it rises. With dryer dough, this is not required. I would begin with the recipe and then start expirementing. If you want to track expirements with the amount of ingredients in the recipe, you can use the baker’s percentage, which measures the ratio of other ingreadetants by weight to that of the flour. Of great inportance is the dough hydration, which is the baker’s percentage of water. Once I am done kneading, I clean the large mixing bowl the dough was in, coat it with olive oil, then add the dough back to the bowl.      Then I move the dough to my bread proofing box and wait a while. You can easily make bread without a proofing box, but see more information about them here. I either try to fold the bread every 30 min to 1 hour, if I am doing a particular wet dough as this helps the gluten form better, or I just let it sit without attention. Like the recipe says, I wait anywhere between 2 and 5 hours as I watch it occasionally to see how it is rising. Once I am convinced it has risen well, I do the short knock back kneading for around 2 minutes, then I divide the dough into two halves, shape it, and either add it to a bread loaf pan or a brotform. Often I will do both to make a round artesian loaf and a sandwich loaf.      Then I let it rise in the proofing box for another 2 to 4 hours until it has risen. Then I preheat the oven and bake the bread. I spray the top of the bread with water (olive oil could also be used) and cut slits in the dough to let the air escape as it rises. I usually start the oven off at 500°F for the first ten minutes to help the bread rise quickly. Then I reduce the temperature to 425°F and add aluminum foil to prevent the crust from hardening as much as it finishes cooking for 15 to 20 more minutes.      The finished bread is great while it is still hot. The second loaf can be frozen if you don’t plan on eating it for a few days. The bread will last for about a week before it becomes stale. Slightly stale bread works great for french toast (even better than fresh bread in fact since it keeps together better as it soaks up the egg/milk mix); also any leftover bread which is stale can be blended into bread crumbs and stored in the freezer. For bread crumbs the bread either has to be stale or toasted in order to blend into good bread crumbs.      Waffles  My favorite waffle recipe is by far the Classic Sourdough Waffles or Pancakes from King Arthur Flour. These Waffles have a great sourdough taste to them. I have found that you can leave excess batter in the fridge and cook it subsequent days without issues. I would try to eat it up within three days though. I also found that the sourdough flavor is slightly stronger each day.         Bread Proofing Box  I wanted to try using a proofing box to see if it would improve the quality of my sourdough bread. However, I didn’t want to by a proofing box if it would not make a big impact. So when I was thinking of a final group project idea for ECE551—Digital System Design, I decided to make my own “Smart” bread proofing box with temperature and humidity control, built-in automatic schedule, and a web interface. The project was great fun and the final report can be found here. As part of the project, I also experimented with making bread with the bread box. I found that the flavors of the bread were better when the bread box was used. At this time I also decided to stop using sour salt in my bread, since I could get great natural sour flavors in my bread and I didn’t want to cover them up with the sour salt. The bread proofer we made for the class worked great, but I ended up getting a commercial bread proofer mostly since it can fold down easily and was made out of sturdier materials. The commercial bread proofer does lack a few of the features from the one I made. It doesn’t have the proofing scheduling/timing, humidity control, or a web interface. On the flip side the commercial one works well and folds down nicely. However, its heating element works much better than the heat lamp I used. The heat lamp would dry out the top of the dough.   This video shows the bread proofer I made in action.               ","categories": ["Life"],
        "tags": ["Food"],
        "url": "/life/sourdough-bread/",
        "teaser": "/assets/images/bread-header.jpg"
      },{
        "title": "PhD Dissertation and Artifacts Available",
        "excerpt":"My dissertation titled SNACC: The Scaled-up Neuromorphic Array Communications Controller is now available at https://trace.tennessee.edu/utk_graddiss/5843/.   I have also made available two general-purpose IP blocks written in VHDL that I created as part of my dissertation work. They are a custom Aurora acknowledgment automatic repeat request design available at https://github.com/Geekdude/aurora-ack and a AXI4-Stream clock converter design available at https://github.com/Geekdude/axi4-stream-clock-converter.   Recommended Citation   Plain Text:   Young, Aaron Reed, \"SNACC: The Scaled-up Neuromorphic Array Communications Controller. \" PhD diss., University of Tennessee, 2020. https://trace.tennessee.edu/utk_graddiss/5843   Bibtex:   @PhdThesis{y:20:snacc,     title       = {SNACC: The Scaled-up Neuromorphic Array Communications Controller},     author      = {Aaron Reed Young},     institution = {University of Tennessee},     month       = {May},     year        = {2020},     url         = {https://trace.tennessee.edu/utk_graddiss/5843/} }  ","categories": ["Research"],
        "tags": ["Neuromorphic Computing"],
        "url": "/research/dissertation/",
        "teaser": "/assets/images/phd-teaser.png"
      },{
        "title": "Moving Website from web.eecs.utk.edu to Github.io",
        "excerpt":"After graduating last May, I decided to migrate my website from hosting on the EECS servers at UTK to GitHub Pages. I always planned on migrating to GitHub pages post-graduation; that was one reason I chose to build my website using Jekyll. GitHub pages can host any static website, but it is designed to build and work with Jekyll sites automatically. I wanted to migrate my website properly with correct redirection and Search Engine Optimization (SEO). Overall the process was relatively easy, but I did run into some issues, primarily with the customizations I have done to the website. This post will cover the overall steps I took and also the issues and workarounds I encountered.   Moving the site to GitHub  Previously I have been hosting my repository as a private repo in bitbucket. I decided to move the repository to a public GitHub repo since I used GitHub pages for hosting. At first, I tried just to move my repository over directly and use GitHub’s built-in building of Jekyll. However, this auto-building feature only supports a subset of Jekyll plugins. The plugin I use for pagination, Jekyll Paginate V2, is not one of the supported plugins. This limitation means that the built-in building would not work for my site without changing the plugins I use. Therefore, I turned off the automatic Jekyll building by adding a .nojekyll file to the root of the repository.   I then tried to build the site automatically with GitHub Actions, but I had trouble getting the site to build using a docker image with the Jekyll build action. While looking into the build issues, I realized my website would not build with the newest versions of Jekyll and Minimal Mistakes. By finding a copy of the Gemfile.lock file I used to build the website with last, I was able to roll back to the old versions and build my website correctly again. Again, the issue was related to the custom template files I created to use Paginate V2 for my home pages and article pages. The trouble with Jekyll and extension versions leads me to an important lessons learned: Store the Gemfile.lock in version control so that you can diagnose issues caused by building with newer versions of the tools and extensions. I was lucky and could just pull the lock file from my backups which saved me extra debugging time.   Since I like the way my website looks and I didn’t want to spend extra time porting my template to the newer version, I decided to keep building the website locally and uploading the static site for hosting. Luckily GitHub Pages supports this flow. The static website can be uploaded to the main branch or a branch called gh-pages. Actually, the branch and path can be chosen under Settings -&gt; Options -&gt; GitHub Pages, so any location can be used for the source. At first, I tested uploading the built website to the main branch, and the website was hosted as expected. But I then decided to host the website’s source in the main branch and a gh-pages branch to host the built website. To make the whole process simpler, I wanted to update my deploy script to build and deploy the website to a branch in the repository. At first, I was hesitant to store the built files in the repo, but since the files are in an orphaned branch, they are separate from the main branch. The orphan branch (as opposed to a folder in the main branch) will make cleaning the files up easier, if they get too large.   Luckily updating the build script was easy since I found a Git Directory Deploy script that I could leverage to copy the built site directory to a separate orphaned git branch. Now my deploy script just calls a customized version of the git deploy script with the variables filled in for my site. Previously it would use rsync to copy the build files to the EECS servers website folder.   Since I decided to use the same local build then deploy setup, minimal changes were needed to the site itself. I updated the site URL in _config.yml as well as updated the previously mentioned deploy scripts. That was all that was required to migrate the site and start hosting on GitHub Pages. The source code for the site can be found at https://github.com/Geekdude/Geekdude.github.io.   Setting up redirection  The next step was to set up redirection from the old URL to the new URL. After looking into redirection, I discovered that I wanted to use redirection with the permanent redirection status (301) which indicates that the resource has moved permanently. From various posts, I learned that a .htaccess file is used on linux servers to specify server-level redirects. I had trouble finding the correct redirect commands until I read the Apache Documentation for Redirect and RedirectMatch. The trouble I ran into was how to correctly map from a folder on a domain to a domain (i.e. /~ayoung48 to /).   The .htaccess file that ended up working properly is   RedirectMatch 301 \"/~ayoung48/(.*)\" \"https://Geekdude.github.io/$1\"   This redirect match command would correctly redirect pages from the old site to the new site. I will leave up this redirection until I lose access to the UTK servers.   Search Engine Optimization  Since the redirection was setup in the previous section, updating the search optimization is easier. I added the new site to Google Search Console and Microsoft Webmaster Tools. There did not seem to be an explicit place to let them know about the move1, but with redirection and having both sites listed under my account, I hope their web crawler will figure it out. I do not even need to redo the website verification method since it was already done for the old address and the verification files are still in place.   Moving Comments  Moving comments with Discus was easy as well. Since I have 301 redirection set up, I could just go into Admin -&gt; Moderate Comments -&gt; Tools -&gt; Migration Tools and use the Redirect Crawler. Then Discus ported the two comments I had on my old site to the new site. I also discovered that I am not getting email notifications when people comment. I will need to look into why. The settings are correctly set.                  There was one for google but it required Domain level property types, not URL Prefix like I am using. &#8617;           ","categories": ["Tech"],
        "tags": ["Website"],
        "url": "/tech/moving-website/",
        "teaser": "/assets/images/website-move-teaser.png"
      },{
        "title": "Dr. Dean featured in PC Gamer",
        "excerpt":"My PhD advisor, Mark Dean, was recentally featured in PC Gamer for his work on the first IBM PC. Read the article at https://www.pcgamer.com/dr-mark-dean-interview-ibm-engineer/.  ","categories": ["Tech"],
        "tags": ["News"],
        "url": "/tech/dean-pc-gamer/",
        "teaser": "/assets/images/dean-pc-gamer-teaser.png"
      },{
        "title": "Getting started with Ctags with Vim and Git.",
        "excerpt":"Ctags with Vim are incredibly useful for quickly navigating around code. With them, Vim can jump between symbols to quickly go to the definition of symbols or find keywords within the document. Ctags + Vim end up working very similar to how Visual Studio Code or other IDEs “go to definition” and “find all symbols works.”   From the ctags website:     Ctags generates an index (or tag) file of language objects found in source files that allows these items to be quickly and easily located by a text editor or other utility. A tag signifies a language object for which an index entry is available (or, alternatively, the index entry created for that object).    Ctags with Vim   Ctags can be annoying to use because you have to run the ctags program to generate a tag file, which then needs to be found by Vim to be used. If the code changes, then the tag file will need to be re-updated. One way to set up ctags is to map a Vim keypress to generate the ctags. In my vimrc, I set it up to regenerate ctags when I press &lt;F5&gt; while not in a  file. More on the ctags command later.   \" Set f5 to generate tags for non-latex files augroup TexTags autocmd! TexTags autocmd FileType tex let b:latex=1 augroup end if !exists(\"b:latex\")     nnoremap &lt;f5&gt; :!ctags -R&lt;CR&gt; endif   In addition to generating the ctags, you also need to tell Vim where to find the generated tag files.   \" Ctags search set tags=./.tags;$HOME   This tags string says to search for the file .tags in the current file’s directory and recursively search upward until the user’s home directory is reached.   Ctags with Vim and Git   However, I do not manually generate the ctags file with &lt;F5&gt;. I store all my source code in Git, so instead of manually generating the ctags file, I use githooks to automatically generate the ctags files whenever I checkout, commit, merge, or rewrite. To setup githooks to be added automatically to newly cloned or created git repos, you make a git template with the correct files. The git template will be copied into the new repositories .git folder on creation.   First, setup the git_template structure.   ~/.git_template/ └── hooks     ├── ctags     ├── post-checkout     ├── post-commit     ├── post-merge     └── post-rewrite  1 directory, 5 files   ctags:  #!/bin/sh set -e dir=\"`git rev-parse --show-toplevel`\" trap 'rm -f \"$dir/.$$.tags\"' EXIT ctags -R --tag-relative --extra=+f -f\"$dir/.$$.tags\" --languages=-javascript,sql mv \"$dir/.$$.tags\" \"$dir/.tags\"   post-checkout:  #!/bin/sh dir=$(git rev-parse --git-dir) $dir/hooks/ctags &gt;/dev/null 2&gt;&amp;1 &amp;   post-commit:  #!/bin/sh dir=$(git rev-parse --git-dir) $dir/hooks/ctags &gt;/dev/null 2&gt;&amp;1 &amp;   post-merge:  #!/bin/sh dir=$(git rev-parse --git-dir) $dir/hooks/ctags &gt;/dev/null 2&gt;&amp;1 &amp;   post-rewrite:  #!/bin/sh dir=$(git rev-parse --git-dir) case \"$1\" in   rebase) exec $dir/hooks/post-merge ;; esac   You also have to add the template to your .gitconfig.  [init]     templatedir = ~/.git_template   Now more about the ctags command. To use the command, you first have to install ctags. I use exuberant-ctags which can be installed with sudo apt install exuberant-ctags on Ubuntu or Debian based Linux. The ctags command I use is   ctags -R --tag-relative --extra=+f -f\"$dir/.$$.tags\" --languages=-javascript,sql   The -R recurses into directories. The --tag-relative sets file paths relative to the tag file. The --extra=+f includes the entry for the base file name of every source file. The -f specifies that the tag file should be saved at the root of the git repository as .tags. Finally, the --languages removes javascript and SQL from the languages which get tagged.   As mentioned before, this template will only apply to new git repositories; therefore, I also created two fish functions to reload git hooks based on the git template. One file reloads hooks in a git repo without submodules, and the other one recursively updates the hooks of all submodules.   git-reload-hooks.fish:   function git-reload-hooks --description 'Reload git hooks' rm -f (git rev-parse --git-dir)/hooks/* git init end   git-reload-hooks-all.fish:   function git-reload-hooks-all --description 'Reload all git hooks'     git submodule foreach --recursive 'rm -f $(git rev-parse --git-dir)/hooks/*;git init' end   If all of this seems like a lot to setup, I recommend storing all your linux dotfiles in a git repository with a script to symlink the files to the right location. I plan to create a future post with more detail on how this is done.   References  I followed https://tbaggery.com/2011/08/08/effortless-ctags-with-git.html when I was setting up ctags for the first time. I have since modified my setup to work better with git submodules and other edge cases.   ","categories": ["Tech"],
        "tags": ["Git","Vim","Ctags"],
        "url": "/tech/ctags/",
        "teaser": "/assets/images/ctags-teaser.png"
      },{
        "title": "ASCR Workshop on Reimagining Codesign Whitepaper",
        "excerpt":"My whitepaper titled Emerging Heterogeneous Systems Provide Great Opportunities for Codesign for the ASCR Workshop on Reimagining Codesign was accepted.   All the accepted whitepapers can be found here.  ","categories": ["Research"],
        "tags": ["Heterogeneous Systems"],
        "url": "/research/ascr-reimagining-codesign/",
        "teaser": "/assets/images/ascr-reimagine-teaser.png"
      },{
        "title": "Happy Mother's Day",
        "excerpt":"Happy Mother’s Day! Below is a blender animation I made a few years ago after following the Blender Guru’s Donut Tutorial.               ","categories": ["Life"],
        "tags": ["Blender"],
        "url": "/life/happy-mothers-day/",
        "teaser": "/assets/images/blender-donut-teaser.png"
      },{
        "title": "Brother's Acting Demo Reel",
        "excerpt":"I helped my brother Preston edit together an acting demo reel using DaVinci Resolve. Check it out on his website at https://volweb.utk.edu/~pyoung15/acting/ or watch it below on Youtube.               ","categories": ["Life"],
        "tags": ["DaVinci Resolve"],
        "url": "/life/prestons-demo-reel/",
        "teaser": "/assets/images/demo-reel-teaser.png"
      },{
        "title": "Dr. Dean to Serve on National Artificial Intelligence Research Resource Task Force",
        "excerpt":"My PhD advisor, Mark Dean, was recentally selected to server on the National Artificial Intelligence Research Resource (NAIRR) Task Force.  Read more about it at https://www.eecs.utk.edu/parker-dean-to-serve-on-national-artificial-intelligence-research-resource-task-force/.  ","categories": ["Tech"],
        "tags": ["News"],
        "url": "/tech/dean-nairr/",
        "teaser": "/assets/images/dean-nairr-teaser.png"
      },{
        "title": "Changing from Disqus to Utterances",
        "excerpt":"I have been having an issue where Disqus would not send me any emails when new comments are posted to the website. After going through all the settings in Disqus, changing my email address, and testing adding comments using a guest login, I could not figure out how to get email notifications to work. Because of this, I changed to using Utterances for my website’s comments. Utterances’ is a great choice since it is open-source and uses the issues tracker of the GitHub repository to store the comments. Each page is its own issue, and the comments are added to the issue. Now email notification works just like GitHub’s issue email notification system. The main downside to Utterances is that you have to log in to a GitHub account in order to post a comment. I liked the flexibility that Disqus provided in terms of login types and guest posting, but I need a commenting system that notifies me when new comments are added, and I like that comments are stored as issues in the issue tracker.  ","categories": ["Tech"],
        "tags": ["Website"],
        "url": "/tech/changing-to-utterances/",
        "teaser": "/assets/images/utterances-teaser.png"
      },{
        "title": "Personal Knowledge Base and Productivity Presentation",
        "excerpt":"Introduction   Personal Knowledge Base and Productivity Presentation Slides   TLDR: Look through the slides, and then read my commentary below for more information on interesting looking slides.   In October last year, I gave a presentation to my group at ORNL on my productivity/note-taking/engineering-notebook system. Another word for this system is a Personal Knowledge Base (PKB) or a second brain.1 This system is inspired by many different systems, and I find it extremely valuable while conducting research. If you are a knowledge worker or student, you will likely find this system useful also. In lieu of a regular written article style blog post, I decided to try something different; I will provide the slides for you to look at, then write my commentary for each slide below for you to read along with the slides. This will save me some effort in restructuring the content of the talk, and it is helpful to look at the pictures that go along with the writing. So for each slide, I will have a section of written commentary on that slide, and you can progress through the slides as you are reading the corresponding section. In the slides, there are many links to internal and external information. Any blue text is an internal link to another location in the slide deck, and any purple text is an external link for more information.   So without further ado, welcome to my written presentation on productivity.   Part 1: Building a Second Brain   In this first part, I talk about the most influential system on my own system, that of Building a Second Brain from Tiago Forte.   Slide 2 — Information Scarcity vs Information Abundance   We now live in a time of information abundance.2 Much of the challenge in a knowledge based field such as research or academia comes from the overwhelming explosion of information that is now readily available in digital formats. If the bodies of written wisdom in books were not overwhelming enough, you now have the entirety of the internet, electronic document repositories, email, multiple messaging applications, many code repositories, and a plethora of tools and open-source programs/libraries. Most of this talk aims at techniques that can be used to avoid drowning in this abundance, and instead, of how to use this abundance to your advantage to thrive and become more productive.   One of the challenges to overcome is to stop living like information is scarce and start living like it is abundant. This means that not everything can be watched or listened to, so you have to be selective about what you give your attention to and be OK with putting it down early if it is not as helpful as your thought. It means that information can be shared instead of hoarded. With scarcity, you could miss out on important information and potentially consume the entire available knowledge on a topic; with abundance, there is no end to the stream of information, and you dip in and out of the stream as needed and collect the best information into your own pool of knowledge.   The amount of knowledge that must be managed in daily life is overwhelming, and the solution to thriving instead of sinking in the abundance is to build a second brain or personal knowledge base. This second brain will be a place for you to store and process your thoughts along with the abundance of external information.   Slide 5 — Building a Second Brain   This next section of the talk is all about the concepts found in Building a Second Brain; although I haven’t taken the course, I have been learning about Building a Second Brain (BASB) through free online resources made available from Forte Labs. There are many links on my slides, and they lead to useful resources or back to the places the information came from. The Miro Board and the Illustrated notes are both great resources created by prior students of the class, but it helps to watch some of the free videos or read some of the free articles to understand the concepts. I’m also greatly looking forward to the upcoming book on BASB.   For each of my slides on a note-taking method, I will provide resources for further reading and key concepts from the method. I am unable to go into too much detail on each method, but I wanted to cover the concepts that are the most helpful to me.   This slide covers the major ideas of BASB. As presented in the Miro Board, I broke the content down into the steps of the productivity pipeline, which are C.O.D.E., Capture, Organize, Distill, and Express. For each of these steps, Tiago presents a concept, technique, and exercise for the step. The place where all these steps take place and where all the information is worked on is within the second brain. On this slide, the blue links take you to another slide in the presentation.   The key concepts listed in the bullets are the previously talked about information scarcity vs information abundance mindsets and the capture criteria of “capture what resonates with you.” The things that you find interesting or grab your attention are the things you should capture in your second brain.   Slide 6 — Building a Second Brain—Capture   The first step in BASB is to have universal capture, and it is about collecting all interesting information. Anything you consume or create, or learn about should be able to be easily captured in the second brain. The second brain should have a universal inbox where anything that captures your attention and resonates with you can be frictionlessly added to the second brain inbox. The capture needs to be frictionless, otherwise its tempting to be too lazy to capture the information, or the capture process will be too slow for live capture. For this step, don’t worry about where the information will go; just focus on having a method of capture that takes no effort.   Slide 7 — PARA   The second step of organize, is about where to put all the things that you have now captured in your universal inbox. The method used is PARA. In addition to the main four locations, I always include an additional 0 as the inbox where everything can be initially placed before it is organized where it should go. By having a capture place, you don’t have to think about where something should go as you are capturing it.   The key principles of PARA are that you leverage the magic number 4, which is easily in the working memory of humans. You have four categories, and each category tree should only have a depth of 4. The second idea is that you mirror this PARA structure across all the applications you use. Even if you can’t use only one program or file system, you can still replicate the same general structure across each. The last idea is that it separates actional items from non-actionable items.   The categories of PARA are Projects, Areas, Resources, and Archive. Projects have a deadline, and they are the things you are actively working on. In my case, they are the active projects that I am currently working on. The scope of my projects is longer running (multiple years), but I can still keep all the information on that project in one place where I will want it in the future. This has helped me to context switch between projects even when I have to put them down for weeks at a time.   Areas are categories of responsibility that are never-ending. These are obligations and things you must do but have no end, such as performance reviews, calls for papers, and weekly reviews. I also put notes on working with particular vendors in the areas section.   Resources are things that you want to keep around for reference, things that are still generally relevant, but you have no project or areas using them. (If a project or area was using the note, then it would go there instead.) I keep my notes on academic papers here, as well as general notes.   Archive is for completed projects, areas that are no longer relevant, and notes that you want to keep but you don’t want in the resources category. If something had no future use, then you can remove it, but I tend to the digital hoarder side, so if I think it had future potential use, but I don’t want to have it in the main categories, I will move it to archive. In my archive, I also have the subfolders: projects, areas, and resources. That way, when I’m looking for an archived item, I can find it in the same PARA structure.   When naming PARA folders, I suggest naming them 0 Inbox, 1 Projects, 2 Areas, 3 Resources, and 4 Archive, so that the order is maintained when sorted. PARA is a useful and convenient organization method, and it places the things that you access more frequently at the top, with things you are less likely to use at the bottom. Using search to look in resources and archive is very helpful, and for these categories, I mostly access them via the search bar, whereas projects and areas I normally navigate to without search.   Slide 8 — PARA   This image shows how I use the same PARA structure in multiple places. This helps me stay organized and switch between programs with less friction.   Slide 9 — Progressive Summarization   We are now in the distill part of CODE. In this step, the notes you have captured and organized are refined into more useful and beneficial notes. I use the distill step to both progressively summarize notes and also to create index pages and build the internal linking between notes. Index pages are pages which summarize a topic or provide additional structure by linking to other pages. Index pages are also a landing page for knowledge on a particular topic.   For progressive summarization, each time you touch a note, you try to make it easier for your future self to skim next time. So each time you look at it, you can add another level of the summarization. The idea here is that the summarization happens over many passes of working with the note. Each time you touch a note, try to leave it a little better than you found it. Add more links, summarize, or add clarification to the notes.   Another idea is that a wall of text is hard to read, but text with bolding and highlighting, text that has the main points pulled out, is easy to skim and to remind yourself of the content quickly without having to do a full read. This allows notes to be reviewed by yourself very quickly at a glance.   The basic five levels of progressive summarization are shown in the slide. For more information on progressive summarization, see Progressive Summarization: A Practical Technique for Designing Discoverable Notes   Slide 10 — Progressive Summarization Landscape   This slide shows the landscape of notes which have been progressively summarized. Each node is at a different point of the summarization process, with untouched nodes still lower in the stack, but certain high-quality, relevant notes have been through all the steps. This makes a landscape of knowledge with the most relevant notes standing out amongst the others.   Slide 11 — Intermediate Packets   With Intermediate Packets, we are now in the express part of CODE. Here is where notes are mature enough that you create something from them. My blog posts represent completed packets, but you can also have progress reports, one-slide summaries, and many other types of intermediate packets. An intermediate packet is just the encapsulated work where you create something that can be shared with others. It goes along with the saying, “We only know what we make”, so by creating content, you learn and create something of value to share.   Slide 12 — Ways of Creating Intermediate Packets   This slide covers the activities that result in intermediate packets.   Slide 12 — Kinds of Intermediate Packets   This slide shows additional examples of intermediate packets.   Slide 13 — Divergence and Convergence   Now that we have covered the tools to capture, organize, distill, and express, let’s look at the complete process as a whole in more detail. From the starting point, you explore and capture and learn all you can. This is the divergence of information. Then when you distill, you start to exploit what you have learned and condense it down into something that you want to express and turn into a final deliverable. This pattern goes along with the idea of exploration vs exploitation found in the multi-armed bandit machine learning problem. Each project you work on should have a similar pattern of breathing in a breadth of information and then breathing out a distilled, condensed deliverable from the information you gathered.   Slide 14 — Just-in-Time Project Management   Also known as the being a lazy project manager.3   I like to combine this idea with lazy note-taking and other just-in-time activities, like lazy progressive summarization. Basically, you do the work when you need to, while reusing, and rehashing previous research and effort. Lazy performance reviews are built from lazy quarterly reports which are built from lazy weekly updates, which are built from a lazy daily log. Basically, at each step, I reuse parts from the previous step so that no part of the process is all that difficult. I lazily improve my notes as I work with them. Each time I don’t put in that much effort to improve them, but over time they are very valuable. More in line with the lazy project cycle, the artifacts from previously completed projects are then recycled back into the second brain to become useful components for future projects.   Slide 15 — 10+1 Principles for Building a Second Brain   These are the 10 (plus a bonus) principles for building a second brain. I think they are fairly clear from the slide. These are the core ideas for building a second brain. For more details, listen to Tiago’s Podcast or read The 10 Principles of Building a Second Brain.   Slide 16 — Solution to Information Overload   Taking the second brain concepts and CODE process together, the second brain becomes a solution to information overload where you repeatedly follow the steps on this slide. These steps are similar to the process of Getting Things Done (GTD).   Part 2: Other Personal Knowledge Base Systems   Now I transition to talking about other systems that have had an influence on my own system.   Slide 18 — Commonplace Book   The main takeaway from this system is that it is a common, central resource for everything. This has inspired me to store everything I find meaningful in my PKB. The other key idea is to always have a way to be able to take quick notes and add to the commonplace book. The way I incorporate this idea, is that I used to carry around a moleskin notebook, and I still have an active bullet journal, but now since I use OneNote heavily, I can just add a note from anywhere from my phone to my OneNote inbox. The last key concept is to use the commonplace book to keep from forgetting thoughts and ideas. “A thought you don’t write down is as good as a thought you never had.”   Slide 19 — Bullet Journal   The way I rapidly log my daily engineering journal is very similar to the Bullet Journal method. The advantage of the digital OneNote version is that it is easy to create links from the bullet to another page with more detail. (Although the same could easily be done on paper with a page reference at the end of the line.) I also keep a personal analog bullet journal, and I am now on volume 4.   One other tidbit I found useful is that after I finish a bullet journal, I can copy the index into OneNote so that when I am searching for a topic, I find the item in the index and know where to look in the physical book. Another interesting idea from Bullet Journal, is that you can create different “Collections,” which are just a fancy way of saying pages with a particular layout and use. Since these collections are created on a blank sheet of paper, the concepts can translate to any medium. The default collections presented by Bullet Journal are very useful for pen and paperwork, but the same types of tools might be implemented automatically by a digital tool or can be included as pages in the digital tool.   Slide 20 — Zettlekästen (Luhmann’s Slip-Box)   The Zettlekästen was a second brain created entirely using index cards for notes. These notes then had many internal references to other notes, which built up a huge web of knowledge. Related notes are linked to other related notes. Although the filing cabinets full of notes are impressive, this system translates well to digital systems where hyperlinks can be made between notes. A key of this system is that you need a unique identifier for each note so that you can create links to it. The system is valuable from the emerging connectivity between ideas. Each idea should be atomic and stand on its own, with one idea per note. The system is expandable in any direction and represents the wavefront of your knowledge in each area.   Another key idea is that there are multiple types of notes and that each node should be handled differently and used differently. These note types are:      Fleeting Notes — Quick notes to remind you of an idea, they will be turned into a permanent note and then discarded.   Literature Notes — Notes on a piece of writing that are tied to the bibliography of the source for future citing.   Permanent Notes — Notes in your own words that are refined so that they are atomic and can stand alone. These are added to the slip-box and never removed.   This system was created with writing academic papers in mind, and the book How to Take Smart Notes goes over the writing method as well as the Zettlekästen design.   Part 3: Note Taking Tools   In this part, I go over a comparison of methods for taking notes and the tools that could be used for note-taking.   Slide 22 — Note-Taking Comparisons   These slides are a series of versions of statements with different note-taking styles compared. In each case, I think both sides are useful and can be used together.   Knowledge Network vs Knowledge Hierarchies   This is comparing a more traditional index as you would find in a book with the web of links you would find on a wiki page. I think both organizational methods are good. A hierarchy is good for locating a note manually, and a web of links is great for representing connections between notes. In my system, I use OneNote with a PARA organizational hierarchy for placing notes in a notebook, then I make heavy use of internal links and index pages to re-represent information and connect related information together.   Search vs Locate   Notes can either be searched for using the search bar of an application or they can be located based on their location in the notebook. As I mentioned in the PARA section, I used both methods. When I am switching contexts between projects, and I know where the notes are located, I navigate to them directly by clicking on them in the hierarchy. If I’m looking for a resource page, I will jump to it quickly by searching for the name of the resource. Sometimes I do both; I search for an index page by name, then I click on the link to the place I want to go. Other times I’m not sure what I am looking for, and I leverage the power of search to look for keywords on a page where I’m not sure what I will find. With these kinds of searches, I’m sometimes surprised that I have notes on a topic that I forgot about.   One Note per Source vs One Note per Idea   How much information should be on each note? Again the answer depends. For myself, if I am taking notes on a resource, I will create one note per source of information. Then as I process the ideas, I will switch to one note per topic. If an idea is a big idea or stands alone, I will then move it to its own note. By in large, I have medium-length notes and try to keep the note length to what can be easily scrolled through. If a note is too long, I will look for ways to break it up. I don’t often mind notes being short, but I don’t want to fill my notebook sections with too many small notes.   I think the best solution to this problem is to reduce the granularity at which you can create internal links. OneNote almost has the right idea here with the ability to create internal links to paragraphs; however, the implementation is bad, and these kinds of links are fragile. Most systems support linking to a heading, this is generally still close enough to get you to the right location in the notes. The key here is that the link between content should make it obvious to what you are linking to. If you take long pages of notes, then a link at the page level is less useful than a link to a header or to a paragraph.   Fluid vs Structure   The idea here is whether one should have a strict structure or template for notes, or if each note should have a fluid ad hoc design. Here again, I tend to the middle ground, where I found that for some types of notes, it is useful to have some common components; but by in large, I am fluid on a note-to-note basis, where each note builds up based on the content of the note. Then if I want to restructure the note later, I can without too much difficulty. So, in general, I start with a black page when I make a new note. Some specific pages, like my weekly reports or bibliography notes have a more rigid structure.   Slide 23 — Consideration for a Note-Taking Program   Here I list of what I think should be important things to consider when you are selecting which note-taking program to use. As an example, I will go through the list with my current program of choice, OneNote.      OneNote can easily capture any type of information, from typed notes to handwritten notes to webpages to whiteboards.   OneNote has powerful search capabilities and can even search inside images and handwritten notes.   OneNote is available on Windows, Mac, Android, iPhone, and via the web. So I can access it from anywhere.   OneNote is backed up locally with the windows client and on the cloud.   OneNote can encrypt certain notes with a password.   OneNote has a default structure similar to physical notebooks with a Notebook → Section Group → Section → Page hierarchy. Other structures can be created via hyperlinks.   OneNote can easily hyperlink to websites, files, sections, pages, or paragraphs.4   OneNote makes it easy to move or reformat notes.   This is one of the areas where OneNote does a bad job. It is difficult to import or export notes from other note-taking apps. OneNote does work OK with the rest of the office suite, and pages can be shared by email.   Again here, OneNote is so-so. Attached files can be easily opened from OneNote, but the notes themselves cannot be edited directly by an external program.   So based on these criteria, OneNote gets an 8 out of 10 with a few areas it could improve on. OneNote is still my program of choice because of the ease of capture, ease of restructuring and reformating, power of the hyperlinks, and because of their unmatched support for the pen and handwritten note-taking with all the other features as well. I won’t claim that OneNote is the best for handwritten notes, but it does seem to be the best for a hybrid of handwritten and typed notes.   Slide 24 — Note-Taking Programs   Here is my list of good note-taking programs. From this list, the ones I have used heavily are the paper lab notebooks, VimWiki, and OneNote. I have also used Evernote to a lesser extent. As mentioned prior, I keep a Bullet Journal and use OneNote for my digital notes.   The other programs on this list are highly recommended by the note-taking community, and I have installed and tried out most of them. I put my short notes for each one on the slide. Of this list, I am keeping a closer eye on Obsidian and Notion as both are gaining a lot of traction in the note-taking community. Obsidian is my favorite markdown-based note-taking program, and Notion is my favorite collaborative system.   Part 4: My Personal Knowledge Base System   Here I go over my own personal system in more detail.   Slide 26 — My Personal Knowledge Base Story   This slide goes over the progression of my PKB over time. I first started with a custom duct tape folder with many nerdy features, which had a clipboard with my weekly task sheet. That is what I used in high school to keep track of academic work.   For my personal journaling/note-taking, I started with a more traditional long-form journal in volume 1; then, for volume 2, I switched to a bullet journal style. Now volume 4 is still using the bullet journal style. I don’t use the handwritten notebooks as much, so it takes a few years for me to fill one up, but I like to still have it and use it.   Next, during internships, I used a physical engineering notebook where I kept all my notes. This worked well when I was primarily only working on one project at a time, and it took a long time to fill up a single book. I outgrew this system in grad school when I filled up notebooks and had more trouble finding previous notes. At this point I was using multiple three-ringed binders to keep my physical notes.   From there, I switched to VimWiki, which I used in the terminal on Linux, along with my code and paper writing in LaTeX. The problem with VimWiki was that it was harder to capture images, and I still needed to do my visual thinking on paper since I like to think using handwriting. I based my VimWiki on a research structure I found on the internet, then later, I restructured my VimWiki using a Zettlekästen style. At this point, I was also trying out using a Rocketbook with Evernote to handle my handwritten notes.   Finally, we get to where I am now, which is using PARA with OneNote. When I started my new job at ORNL, I switched to a windows machine for my main computer, and OneNote is a supported electronic lab notebook tool. I have greatly enjoyed using it, apart from a few various gripes.   Slide 27 — Demo My Personal Knowledge Base   During the actual talk, I gave a live demo of my note-taking system. Since this presentation is written, I will give more details on my OneNote structure.   I’ve been using PARA with OneNote for work and personal use since I started my current job a year and a half ago. I am a Software Engineer/Researcher for Oak Ridge National Laboratory, so I have many active projects and technical notes to keep track of. My second brain stored in OneNote with PARA organization has been an indispensable resource to me.   My OneNote structure is:      Notebook for Work            Inbox       Journal (Daily log of research activities)                    I use my journal to track tasks I work on and links to meetings and note pages.           I use note hierarchy to organize and minimize notes by year.           At the bottom of the current month, I also keep my todo tasks list organized by project with a todo today, todo next, and todo inbox.                       Notes (General notes. This could be in 3 Resources, but I keep it here since I add to it frequently. Notes in here I find primarily from search)       [SG] 1 Projects                    [SG] Name of first project                            Notes - Name of first project               Meetings - Name of first project               Emails - Name of first project                                   [SG] Name of second project                            ….                                               [SG] 2 Areas                    Various area sections and section groups                       [SG] 3 Resources                    Various resource sections and section groups                       [SG] 4 Archive                    Same PARA structure for inactive resources.                           Personal Notebook            … similar PARA structure           [SG] is a section group.   Tips and Tricks:      Section groups are sorted alphanumerically, so you can use numbers to force a sort order. (i.e., “1 Projects”, “2 Areas”, …)   Use hyperlinks and index pages.            You can hyperlink internally to any paragraph, this is super powerful, and I use it to link notes together in a Zettlekästen like web. I like the mix of hyperlinks and hierarchical sections for organizing notes—the best of both organization methods. Specific index landing pages with hyperlinks can be a view into your notes with a different organization.           Add “- project name” to sections that have names that are the same in different section groups.            This makes it much easier to find the correct section to move an item to when you move a note.           Only put content in one place. Hyperlink if you want it somewhere else. This means I don’t copy notes; I just move them to the most logical place.   I have found that OneNote is less keyboard friendly for key bindings, so I use a stream deck and macros to make it much easier to do common OneNote Operations.   If you are using OneNote on a windows desktop, you can install Onetastic, which adds some missing features in OneNote and the ability to write advanced macros. (I wrote my own macros to make the conversion from my previous second brain in VimWiki to OneNote easier.)   Integration with Outlook is great if your work also uses Outlook for email.   Sending note pages via email also works great.   Pen support with a pen display or iPad is great. So are pictures of whiteboards or screenshots of presentations. OneNote attempts to do OCR on pictures and handwriting with various levels of success. Still, it was good enough for me to rediscover useful information that I had written in cursive using search.   Part 5: Productivity Tools   At this point of the talk, I switch from talking about PKB and start talking about various productivity tools for Software Engineers/Researchers.   Slide 29 — Useful Hardware for Remote Work   On the slide is a picture of my remote work setup. I found it great to have a pen display for digital handwritten notes and for remote whiteboarding. A stream deck is wonderful for easily creating macros. The best part is that since the buttons have a screen, you can see what each button will do. When you switch programs, the macros switch as well. The stream deck tasks some time to manually set up, but it is a joy to use and a great time saver once set up. I especially like using it with OneNote since OneNote keybindings are limited, but with macros, I can leverage alt menu navigation to perform complex functions with a single button press. I recommend having a nice docking station or a desktop computer. In my setup, I have a dual laptop stand and USB Type-C dock, which works with both my personal and work computer.   Update: Now that I am working at the office, I have a similar setup at work so that it is easy to switch from at home and on-site work.   Slide 30 — Stream Deck   This slide shows off some of my custom Stream Deck macro pages for various applications.   Slide 31 — JabRef   JabRef is an amazing tool for managing bibliographies if you primarily write papers using LaTeX with the bibliographies in the BibTeX or BibLaTeX format. I keep my JabRef bibliography in Git, and I have a personal repository and a shared repository within my group at ORNL. I started using JabRef in grad school, and it continues to work well.   Slide 32 — Slack   This slide has some of the main tips for using slack from a slack training I attended.   Slide 33 — Project Management in GitLab   Project management in GitLab, GitHub, or Bitbucket is great, and I highly recommend leveraging the tools to manage projects. One thing that has been useful for research projects is to create a project repo that tracks the project as a whole and can be used to store all the artifacts from the project. The issue board can keep track of tasks and discussions on topics. Separate code repos can be added to the project repo as submodules for organization. READMEs and wikis can be used to keep shared knowledge on the project.   Slide 34 — Weekly Review   I follow a process similar to Tiago’s Guide to doing a weekly review. This process is helpful for me to keep my digital workspace uncluttered.   The next two slides are from Tiago’s Guide to doing a weekly review and highlight the process.   Slide 37 — Weekly Review   This slide gives an overview of the steps to a weekly review, and I would recommend reading and watching Tiago’s material for more details on the process.   Slide 38 — Task Tracking   I would recommend tracking tasks and using a productivity method similar to GTD.   Slide 39 — Time Tracking   I used Toggl Track to track the time I spend on various projects.   Appendix   I included my summary of Tiago’s summary of How to Take Smart Notes.                 The methods can also be referred to as a Personal Knowledge Management (PKM). &#8617;                  The slide is from Building a Second Brain Miro Board and idea of living in the abundance of information instead of scarcity comes from Tiago Forte. &#8617;                  Taken from the idea of The Lazy Dungeon Master &#8617;                  Although paragraph linking is currently too fragile to be relied on, since moving a page to a new location will break the paragraph links. &#8617;           ","categories": ["Research"],
        "tags": ["Productivity","BASB","PKB"],
        "url": "/research/productivity/",
        "teaser": "/assets/images/productivity-teaser.png"
      },{
        "title": "My Favorite Vim Plugins",
        "excerpt":"I use Vim and Visual Studio Code as my main text editors, and I have found multiple plugins to both which add nice features to the respective program. In this post, I will go over my favorite Vim plugins. I install these plugins as part of my dotfiles setup, so it is easy to set up Vim with plugins on each system I use.   Core Plugins   These plugins are ones that I highly recommend and add what I would consider core features to Vim.   vim-commentary   url:  https://github.com/tpope/vim-commentary.git   Comment stuff out. Use gcc to comment out a line and gc to comment out a motion.   vim-gitgutter   url:  https://github.com/airblade/vim-gitgutter.git   Show the file’s git diff status in the gutter.   vim-vinegar   url:  https://github.com/tpope/vim-vinegar.git   Makes netrw work much better. The main addition is - to go up a directory.   vim-unimpaired   url:  https://github.com/tpope/vim-unimpaired.git   Pairs of handy bracket mappings. They allow [ or ] followed by a character to jump forward or backward. ]q is :cnext. [q is :cprevious. ]a is :next. [b is :bprevious. See the documentation for the full set of 20 mappings and mnemonics.   vim-rename   url:  https://github.com/wojtekmach/vim-rename.git   Rename a buffer within vim and on disk. Use :saveas &lt;newfile&gt; to rename the file and remove the old file from disk.   vim-textobj-entire   url:  https://github.com/kana/vim-textobj-entire.git   Provides text objects ae and ie which represent the entire buffer.   vim-capslock   url:  https://github.com/tpope/vim-capslock.git   Press &lt;C-G&gt;c in insert mode to toggle a temporary software caps lock. I used this when I didn’t have a caps lock since I rebound it to control. Now I don’t use this as much since pressing both shift keys is my caps lock.   vim-textobj-user   url:  https://github.com/kana/vim-textobj-user.git   Create your own text objects easily. Used by other plugins.   vim-repeat   url:  https://github.com/tpope/vim-repeat.git   Enables repeating supported plugin maps with ..   vim-surround   url:  https://github.com/tpope/vim-surround.git   Adds surround text mapping with s. I.e., cs\"' to change \" to ' surrounding text.   vim-visual-star-search   url:  https://github.com/nelstrom/vim-visual-star-search.git   Start a * or # search from a visual block.   vim-abolish   url:  https://github.com/tpope/vim-abolish.git   Easily search for, substitute, and abbreviate multiple variants of a word.   calendar-vim   url:  https://github.com/mattn/calendar-vim.git   Open a calendar within Vim. Use :Calendar to open.   vim-exchange   url:  https://github.com/tommcdo/vim-exchange.git   Easy text exchange operator for Vim.   Mappings   cx   On the first use, define the first {motion} to exchange. On the second use, define the second {motion} and perform the exchange.   cxx   Like cx, but use the current line.   X   Like cx, but for Visual mode.   cxc   Clear any {motion} pending for exchange.   Some notes   If you’re using the same motion again (e.g. exchanging two words using cxiw), you can use . the second time. If one region is fully contained within the other, it will replace the containing region.   vim-textobj-sentence   url:  https://github.com/reedes/vim-textobj-sentence.git   Improving on Vim’s native sentence text object and motion.   Taboo   url:  https://github.com/gcmt/taboo.vim   Few utilities for pretty tabs. :TabooRename &lt;name&gt; to rename a tab.   Awesome Added Features   These plugins add cool new features to Vim.   tabular   url:  https://github.com/godlygeek/tabular.git   Vim script for text filtering and alignment. Amazing plugin for aligning LaTeX tables and assignment lists.   ctrlp.vim   url:  https://github.com/ctrlpvim/ctrlp.vim.git   Use &lt;c-p&gt; for a fuzzy finder. Easily search for files.   vim-buffergator   url:  https://github.com/jeetsukumaran/vim-buffergator.git   List, select, and switch between buffers. Use &lt;Leader&gt;b to open the buffer side panel.   tagbar   url:  https://github.com/majutsushi/tagbar.git   Display tags in a window, ordered by scope. Used to navigate quickly within a file. Use &lt;Leader&gt;cc to open.   supertab   url:  https://github.com/ervandew/supertab.git   Perform all your vim insert mode completions with Tab.   undotree   url:  https://github.com/mbbill/undotree.git   Undo history visualizer for Vim. Use :UndotreeToggle to start. This has saved me in the cases where I undid some changes, then accidentally made a new change but wanted to redo it to the previous state.   vim-autocorrect   url:  https://github.com/panozzaj/vim-autocorrect.git   Correct common typos and misspellings as you type in Vim. Note: this plugin is slow to load, but I still find it super helpful and less annoying than Word’s autocorrect.   Cheat.sh-vim   url: https://github.com/dbeniamine/cheat.sh-vim   Browse cheat sheets from cheat.sh directly from vim.   Note-taking   These are plugins that go alone with taking notes in vim. See Personal Knowledge Base and Productivity Presentation for more information on note-taking.   vimwiki   url:  https://github.com/geekdude/vimwiki.git   A Personal Wiki for Vim. I used this as my primary note-taking tool before switching to OneNote. See Personal Knowledge Base and Productivity Presentation for more information on my note-taking system.   vim-zettel   url:  https://github.com/michal-h21/vim-zettel   Vim plugin to implement Zettelkasten with Vimwiki.   fzf   url:  https://github.com/junegunn/fzf.vim   fzf wrappers for vim.   notational-fzf-vim   url:  https://github.com/alok/notational-fzf-vim   Notational Velocity for Vim. It is used to fuzzy search through a Vimwiki.   Writing Tools   These are the plugins that help with the paper writing process in Vim.   vimtex   url:  https://github.com/lervag/vimtex.git   A Vim plugin for editing LaTeX files. This is my favorite of the many LaTeX plugins.   vim-dict   url:  https://github.com/szw/vim-dict.git   Dictionary lookup inside Vim. To lookup a work use the :Dict command.   vim-online-thesaurus   url:  https://github.com/Ron89/thesaurus_query.vim   Online thesaurus lookup. Use &lt;Leader&gt;cs to query and replace the current word.   vim-instant-markdown   url:  https://github.com/suan/vim-instant-markdown.git   Instant Markdown previews with Vim. Use :InstantMarkdownPreview to start.   LanguageTool   url:  https://github.com/vim-scripts/LanguageTool.git   Grammar checker using LanguageTool for Vim.   vim-pencil   url:  https://github.com/reedes/vim-pencil   Plugin to help Vim be more friendly for writing prose. It handles automatic wrapping of text.   goyo.vim   url:  https://github.com/junegunn/goyo.vim.git   Distraction-free writing in Vim.   Coding or Language Support   vim-flake8   url:  https://github.com/nvie/vim-flake8.git   Verify Python syntax with flake8. To use press &lt;F7&gt;.   vim-scala   url:  https://github.com/derekwyatt/vim-scala.git   Add Scala programming language support to Vim.   vim-fish   url:  https://github.com/dag/vim-fish.git   Vim support for editing fish scripts.   DoxygenToolkit.vim   url:  https://github.com/vim-scripts/DoxygenToolkit.vim.git   Simplify Doxygen documentation in C, C++, Python.   vim-gdscript3   url:  https://github.com/calviken/vim-gdscript3   Support syntax for Godot script.   Plugins used for code and colorscheme debugging   vim-HiLinkTrace   url:  https://github.com/gerw/vim-HiLinkTrace.git   Trace highlighting with \\hlt.   xterm-color-table.vim   url:  https://github.com/guns/xterm-color-table.vim.git   Displays all 256 terminal colors with the command :XtermColorTable.   hexHighlight   GVim plugin to highlight hex codes to help with tweaking colors.   Conque-GDB   url:  https://github.com/vim-scripts/Conque-GDB.git   GDB command-line interface and terminal emulator in Vim.   Color Themes   Note: I don’t use these color schemes since I created my own color scheme.   palenight.vim   url:  https://github.com/drewtempelmeyer/palenight.vim.git   Soothing color scheme for vim. Recommended by  https://blog.pabuisson.com/2018/06/favorite-color-schemes-modern-vim-neovim/   vim-one   url:  https://github.com/rakr/vim-one.git   Adaptation of one-light and one-dark (Atom) colorschemes for vim. Recommended by  https://blog.pabuisson.com/2018/06/favorite-color-schemes-modern-vim-neovim/   molokai   url:  https://github.com/tomasr/molokai.git   Molokai color scheme for Vim. Recommended by  https://www.slant.co/topics/480/~best-vim-color-schemes   gruvbox   url:  https://github.com/morhetz/gruvbox.git   Retro groove color scheme for Vim. Recommended by  https://www.slant.co/topics/480/~best-vim-color-schemes   Unused but still might be helpful   These are plugins that I no longer use.   vim-fugitive   url:  https://github.com/tpope/vim-fugitive.git   A git wrapper for Vim.   YouCompleteMe   url:  https://github.com/Valloric/YouCompleteMe.git   A code-completion engine for vim. Note: I don’t use this one anymore since it was a bigger pain to set up than the benefit of using it.   vim-smooth-scroll   url:  https://github.com/terryma/vim-smooth-scroll.git   Makes scrolling in Vim nice and smooth. (Currently, I do not have this plugin mapped.)   vim-qargs   url:  https://github.com/nelstrom/vim-qargs.git   Adds a Qargs utility command to populate the argument list from the files in the quickfix list. Note: The functionality provided by this project has been made largely obsolete in recent versions of Vim.  ","categories": ["Tech"],
        "tags": ["Vim"],
        "url": "/tech/vim-plugins/",
        "teaser": "/assets/images/vim-plugin-teaser.png"
      },{
        "title": "My Dotfiles Storage using Git and Shared Dotfile Repo",
        "excerpt":"Introduction and Dotfile Storage Method   When I started heavily using Linux in college and started customizing and creating configuration files, I quickly needed a way to store, backup, and set up my various configuration dotfiles. A logical place to store configuration files is in git. You can see modifications to the dotfiles, easily commit new changes, and even make multiple branches if you need different versions of the dotfiles. I do not remember the exact tutorial I followed, but a form of the method I used seems to be floating around on the internet in multiple blog posts. Managing Your Dotfiles With Git from 2020 seems to use the same method as me, and credits Using Git and Github to Manage Your Dotfiles from 2012 for the method. Given the age of the older post and how closely his example on GitHub matches the first commit of my dotfiles repo, I would guess that this is the template I followed.   I started with a simple base and then built up the dotfiles and configuration over many years. My dotfiles repository currently has 623 commits and was started in 2016. The original structure from the example can still be found in my setup. Therefore, instead of reproducing the same content as Using Git and Github to Manage Your Dotfiles, I recommend you pause reading this blog and read about this method of storing dotfiles from the original blog post.   Read Using Git and Github to Manage Your Dotfiles   My Public Dotfiles   Now that you have read about the dotfile setup, you know how the dotfile directory is stored in git. A setup script is used to create symlinks from the configuration files in the home directory to point back to configuration files in the dotfiles folder. I have multiple branches in my dotfiles repo. I have one main branch which I use on my personal Linux machines; then I also have a branch, that I used on the University machines at school, a branch that I use on machines at work, and a branch that I use on WSL machines. Each of these types of machines has some deviations from the main branch, but I periodically compare the contents back to the main branch to ensure that all the good features are present in all and that the places they diverge still apply. Since some of these configuration files are specific to work or to my own machines, I decided to make a public dotfiles repo, where I will copy the content of my dotfiles which is not sensitive and which is generic across the branches and that I think will be of general use and interest. This public dotfiles repository can be found at https://github.com/Geekdude/dotfiles. I hope you find my dotfiles interesting and helpful as you create your own dotfiles repository.   As you create your own dotfiles repository, I recommend that you start from a simple base, go through example configurations, and understand what the configuration lines are doing. Then you can copy in the lines which change the configuration in a way that you like. Don’t just use public configuration examples without taking the time to understand what the configuration is doing. You probably do not want parts of my configuration, but some of it you will likely find helpful. I enjoy learning about the features of other people’s configurations and potentially incorporating some of the configurations into my own config. My configurations started simple and grew over time as I wanted my system to behave differently and searched for a solution or saw a cool configuration on the internet or from colleagues. Sometimes I ran into a specific problem and found the solution in a configuration change, then incorporated the change into my configuration files.   Over time I also changed the programs I use, so I have some older configuration files for these programs, even though I do not use these programs as much. I also changed which shell I use primarily. I started with Bash, then switched to Zsh for the added features. Now I am currently using Fish. I still keep my Bash and Zsh config in case I go back to these shells, but I primarily use Fish now. One downside to Fish, is that it is not compatible with Bash, so I still find myself going back to Bash or Zsh frequently to run Bash scripts. I liked the general look I had for Bash, so I kept the same general look as I configured Zsh and Fish.   Now that you know the general dotfile setup and have access to my public dotfiles, I will talk in more detail about my dotfiles, and as I do, you are free to look through the dotfile repository as well. I tried to be clear in the comments of the dotfiles with what the configuration lines do, and in this blog post, I will give broader information about the features and reasoning behind the setup.   README.md   The README file goes over how to set up the dotfile configuration on a new system. From the steps, you can see how the relatively simple process is made more difficult when I need to use ssh keys to pull from a private repository.   makesymlinks.sh   This file creates the symlinks for the dotfiles back to the corresponding file in the dotfiles directory. Over time, this script was also extended to pull and update external submodules, create other additional symlinks, and link symlinks to folders.  This script is run to set up all the dotfiles. I have more submodules listed in .gitmodules than I actually use. That is why I explicitly list the submodules I want in the script. This is primarily done since it is much easier to add a new submodule than to remove it, and I might want to use it again.   Ssh   In my private dotfiles repo, I keep passcode protected and GPG encrypted ssh keys. The makesymlinks.sh script will call the command to decrypt the keys as part of the setup. In practice, I have to add the keys before I can clone the dotfiles directory, and I have to manually set up the symlink to .ssh. That’s is the problem with using a private dotfiles repo; the keys need to be in place before you can clone the repo. I also use a .ssh/config file to configure connections to remote machines. Since the information in .ssh is sensitive, I have not included any of it in the public dotfiles repo.   Git   The git configuration is stored in gitconfig, global git ignores are stored in gitignore_global, and my git hook templates are stored in git_template. I removed the user.name and user.email config items from the gitconfig, but those can be easily added back when git prompts you to set them.   I use Beyond Compare 4 as my main diff tool. I also added other tools with aliases. vimdiff, vimmerge, gvimdiff, and gvimmerge use Vim and GVim respectively to perform the diff or merge. I also found difftastic, which is a better terminal diff than diff and is aliased to difft. My favorite alias, which I use all the time, is tree, which prints out a nice tree view of the git log.  Other aliases are sup which recursively updates all submodules, and sfor is an alias to run a for loop over all submodules. I also have hash which prints out the current git hash, and pwd, which prints out the top-level directory of the git repo. pwd might be a misleading name, but I can remember it easily. Finally, I also have s and d, which prints the status or diff of the current directory respectively.   My template directory has the necessary hooks to set up ctags. These ctags are then used by my Vim configuration to jump to tags in my file quickly. Each time a git operation occurs the, ctags are regenerated, which keeps them up-to-date. My global gitignore_global ignores the generated ctag files. Ctags are further discussed in my Ctags blog post   I like to use fast-forward-only so that I have to explicitly choose to make a merge. That way, it is easier for me to pick to merge or to rebase instead of defaulting to merge.   Fish   I like to use the Fish shell. I think it is a well-designed shell with an easy-to-understand configuration structure. Out of the box, it has many of the features by default that are a pain to set up in Zsh. I was also able to keep the same basic theme as my previous shell configurations. Some of my favorite features are:      Automatic prediction and auto-completion of commands based on previously used commands.   Command completion based on man pages. (I also have a function, fish_add_completions, to generate a manpage from --help output, and reload the man completions so that programs I write can have easy command completions)   Powerful prompt with git status, exit code, time, and the duration of the last command.   Easy configuration with a conf.d for config files and functions for custom functions.   My config.fish has inline comments describing what everything does. One thing to point out is that I call setxkbmap and xcape commands to replace capslock with control/escape and both shifts with capslock.   Inside conf.d I have configuration files broken down by the name of what they are configuring. Any file in this directory will be included in the configuration of Fish. I use ssh_agent.fish to make sure I always have an ssh-agent running.   Inside functions, I have all my custom functions. In Fish, the prompts are just a shell function, so my prompt is defined in fish_prompt.fish and fish_right_prompt.fish. funcedsave.fish combines the built-in funced and funcsave functions.   git-add-submodule.fish adds an existing clone nested repository as a submodule of the parent. git-reload-hooks.fish and git-reload-hooks-all.fish reload hooks based on the git template for either the current repo or recursively into all the submodules.   Something new I am trying is the fisher Fish plugin manager with z for directory jumping.   Tmux   My Tmux config is set up to work with nested local and remote Tmux sessions. tmux.conf specifies the local Tmux configuration and tmux/tmux.remote.conf adjusts the status bar on the remote Tmux session. &lt;F12&gt; is used to toggle the disable/enable of the outer Tmux session to make the inner Tmux session easier to work with.   My Tmux config also has mouse support, plugins to show resource monitoring, and more Vim-like bindings.   I spent a while figuring out the full-color range for both Tmux and Fish, and the color settings at the top of the file seem to work. Since I use MobaXTerm, which has some nifty features that only work if Bash is your default shell, I use Tmux to set my default shell to Fish. That way, Fish is started anytime I start my Tmux session.   In bin, I also have some helpful Tmux scrips. tmuxes opens or connects to a new single Tmux session with the same view as a previous connection. This command only opens a common Tmux session, and multiple sessions will follow the same cursor. tmuxs also opens the same Tmux session, but this time it creates a new view into it so that the cursor does not follow the previous window. tmux_allow and tmux_disallow changes the permissions of the Tmux socket to allow other users on the same machine to connect to the same Tmux session.   Vim   I use Vim and Visual Studio Code as my main text editors. Vim is still the editor that I am the most proficient with and also the one that I have customized the most. With the various plugins I have, my Vim behaves more like a modern tool like Visual Studio Code compared to the vanilla Vi editor. I have and still use Vim as a code editor, word processor, note taker, and documentation writer, and various plugins make it more suited for these tasks. To learn about the plugins I use see My Favorite Vim Plugins. These plugins are installed using pathogen, and the plugins are added as submodules to my dotfiles repo. makesymlinks.sh is used to initiate and update the submodules. Some of the plugins I have modified and the submodules point to my forks of the original repos.   Just like with my other config files, I have inline comments to explain my configuration. However, I will mention here some specific features I have. See my Ctags blog post for more information on how Ctags is set up. I like to use three spaces by default, even though that seems like a chaotic-neutral choice on the alignment chart. I have a check which disables many features when the text file is over 10MB. With this setting, Vim can open humongous text files that break most other text editors. I have a function that automatically strips trailing whitespace from files on save except when editing markdown files since trailing whitespace has meaning in markdown. To help see trailing whitespace in those file formats, I add a · in place of the trailing space. I created my own color scheme, which is a modification of the default color scheme. I also set up proper 256 and GUI colors so that the color scheme looks good.   I added two useful functions for when I edit text. To make line diffing of text easier, and since LaTeX and Vim treat newlines without a blank as part of the same paragraph, I write one sentence per line in the source document.   \" Search for paragraphs that do no end in a newline. nnoremap &lt;leader&gt;l&lt;space&gt; /[.!?]'\\?'\\?\\zs\\s\\+\\ze\\(\\w\\\\|\\\\\\)&lt;cr&gt; \" Join lines and add a newline at the ends of sentences. vnoremap &lt;leader&gt;l&lt;space&gt; :s/[.!?]'\\?'\\?\\zs\\s\\+\\ze\\(\\w\\\\|\\\\\\)/\\r/g&lt;cr&gt;   When &lt;leader&gt;l&lt;space&gt; is pressed, the sentence ending periods that are not followed by a newline are searched for and highlighted. When a visual selection is made, and &lt;leader&gt;l&lt;space&gt; is pressed, the newlines are added to the selected region. This makes it easy to reformat text note in the one sentence per line format to this format. Just use J to join all the lines in a paragraph, then press &lt;V&gt; then &lt;leader&gt;l&lt;space&gt;.   :DuplicateTabpane is useful for quickly duplicating your current window layout to a new tab. Then :TabooRename can be used to rename the tab to a new name.   Here are my keybindings that I use when working with vim-zettel:   Zettel Vim Keybindings  &lt;leader&gt;zs  ZettelSearchMap &lt;leader&gt;zy  ZettelYankNameMap &lt;leader&gt;z   ZettelNewSelectedMap &lt;leader&gt;nv Notational Vim :ZettelNew &lt;title&gt;   Basic Apt Installs   This script is used to update and upgrade apt packages, then install a common set of apt packages that I expect every system to have. Between the dotfiles repo, this script, and Synology drive, I can go from a fresh Ubuntu install to a configure system very quickly. This is useful since I redo my computers frequently or set up a new virtual machine.   Bin   create-thumbnails is used to reduce the resolution of images to create thumbnail versions. I use this for my website to reduce the size of the smaller images in a gallery.   pdf-shrink reduces the size of PDFs by calling other commands with the correct arguments.   esv uses the ESV API to create a terminal program to read or listen to the bible. If you want to use this program, you will first have to generate an API token from ESV.org.   Reorder is a program I wrote and is discussed in the Reorder blog post   Fonts   Here I store all the fonts I want to use across my machines. Then, makesymlinks.sh will create a symlink to the local font location and run fc-cache to cache the fonts so that they are properly installed as locally installed fonts. I did not include the fonts in the public repo since I didn’t want to check the licensing on each font.   Inkscape   I set up Inkscape with the extensions I use. I created my own addition to circuit symbols to make a black box symbol.   Bonus: Zork   No system is complete without Zork, so as a bonus, I install Zork by default.  ","categories": ["Tech"],
        "tags": ["Linux"],
        "url": "/tech/dotfiles/",
        "teaser": "/assets/images/dotfiles-teaser.png"
      },{
        "title": "One Lap of America 2023",
        "excerpt":"Last year, my dad and I competed in Brock Yates’ One Lap of America 2023 driving competition.  This is our second time entering this driving competition; the last time we entered was in 2019. It took a full year until we went on our third One Lap of America (2024) before I got around to processing the videos and photos from 2023.  The transit provided ample time for me to work on editing last year’s footage. Eventually, I caught up, so I edited the footage from the morning when I was taking a break from driving in the afternoon. We got a nice rhythm down. After the morning’s event, I would take the first shift of the transit. Then, when we ran low on gas, we would stop at a gas station, and I would collect the SD cards to download and process the photos and video during the second transition before switching again and driving the rest of the transit for that day.   The One Lap of America official website is onelapofamerica.com.  Since the event was technically a competition, the events were time trials. Each team was scored and ranked based on how fast they completed each event. The event results and rankings can be found on the One Lap of America website under History -&gt; Previous One Laps.    We took my dad’s new 2023 Chevrolet C8 Stingray this year. We were not sure if it would arrive from the dealer in time, so we debated whether to take my brother’s Kia or my Beetle as a backup vehicle. Luckily, it did come in in time for us to prepare the car and get it to One Lap. We finished putting in the recommended miles on the car before it was track-ready during the transit to the start of One Lap.   Our car’s number this year was 37, and we finished 53rd overall. Our goal, however, was not to place well but to have fun, complete the event, and bring the car back home, and we completed all of these goals. The competition was long, but fun.  Each day, we had roughly the same schedule. We would wake up early, around 7, and then head to the track, where we would drive in two events.  After which, we would drive for around 500 miles to reach the next hotel, so we could do it all again.                                                    Map of the 2023 Route.       Picture Gallery                                                                                                                                                                    Day 1 Skid Pad and Autocross, South Bend and Kokomo, Indiana               Day 2 Nelson Ledges, Garrettsville, Ohio               Day 3 Road Atlanta, Braselton, Georgia.               Day 4 Nashville Superspeedway, Lebanon, Tennessee               Day 5 Eagles Canyon Raceway, Decatur, Texas               Day 6 Hallett Motor Racing Circuit, Jennings, Oklahoma.               Day 7 National Corvette Museum (NCM) Motorsports Park, Bowling Green, Kentucky               Day 8 Skid Pad, South Bend, Indiana               ","categories": ["Life"],
        "tags": ["Driving"],
        "url": "/life/one-lap-of-america-2023/",
        "teaser": "/assets/images/oloa-23-car.JPG"
      },{
        "title": "One Lap of America 2024",
        "excerpt":"My dad and I recently completed our third year of Brock Yates’ One Lap of America 2024 driving competition. For some reason, we keep coming back 🤪. I’m just kidding; it is a lot of fun.   This year, I edited the videos and compiled the pictures while in transit. I also started and completed going through last year’s pictures/videos.  This year, we got a nice rhythm down. After the event in the morning, I would take the first shift of the transit. Then, when we ran low on gas, we would stop at a gas station, and I would collect the SD cards to download and process the photos and video during the second transition before switching again and driving the rest of the transit for that day.   The One Lap of America official website is onelapofamerica.com.  Since the event was technically a competition, the events were time trials. Each team was scored and ranked based on how fast they completed each event. The event results and rankings can be found on the One Lap of America website under History -&gt; Previous One Laps.    My dad’s 2023 Chevrolet C8 Stingray has now completed two One Laps of America. Our team number was 55, and we beat our number for the first time, finishing 50th overall. Our goal, as always, was not to place well but to have fun, complete the event, and bring the car back home, and we completed all of these goals. The track route was really nice this year, and the trip overall seemed easier this year than in past years. The time zones usually changed in our favor for the long days and we maintained good sleep starting out.                                                    Map of the 2024 Route.       Picture Gallery                                                                                                                                                                                                                                                                                                                                                                                                                                                       All Transit at 1000x Playback               Day 1 Skid Pad, South Bend and Kokomo, Indiana               Day 2 Motorsports Park Hastings, Hastings, Nebraska                                                                                                                                                                                                                                                                                                                                                                    Day 2 Gallery.                   Day 3 High Plains Raceway, Deer Trail, Colorado                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Day 3 Gallery.                   Day 4 Hedge Hollow Racetrack, Adrian, Missouri                                                                                                                                                                                                            Day 4 Gallery.                   Day 5 National Corvette Museum (NCM) Motorsports Park and Autocross, Bowling Green, Kentucky                                                    Day 5 Gallery.                   Day 6 Pittsburgh International Race Complex, Wampum, Pennsylvania                                                                                                                                                                                                                                                  Day 6 Gallery.                   Day 7 Putnam Park Road Course, Greencastle, Indianna                                                                                                                                                                                                            Day 7 Gallery.                   Day 8 Skid Pad, South Bend, Indiana               ","categories": ["Life"],
        "tags": ["Driving"],
        "url": "/life/one-lap-of-america-2024/",
        "teaser": "/assets/images/oloa-24-cover.jpg"
      }]
